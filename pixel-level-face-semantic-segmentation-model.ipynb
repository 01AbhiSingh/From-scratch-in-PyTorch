{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10779778,"sourceType":"datasetVersion","datasetId":6688632}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport torchvision.transforms.functional as TF\nfrom sklearn.model_selection import train_test_split\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:01:04.516005Z","iopub.execute_input":"2025-03-31T16:01:04.516322Z","iopub.status.idle":"2025-03-31T16:01:04.520932Z","shell.execute_reply.started":"2025-03-31T16:01:04.516297Z","shell.execute_reply":"2025-03-31T16:01:04.520078Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"IMAGE_SIZE = 512\nNUM_CLASSES = 11  \nBATCH_SIZE = 4\nEPOCHS = 10\nLEARNING_RATE = 0.0001\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:01:05.567584Z","iopub.execute_input":"2025-03-31T16:01:05.567875Z","iopub.status.idle":"2025-03-31T16:01:05.572057Z","shell.execute_reply.started":"2025-03-31T16:01:05.567853Z","shell.execute_reply":"2025-03-31T16:01:05.571247Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"DATASET_PATH = \"/kaggle/input/helen-face-segmentation-dataset/helenstar_release\"\nTRAIN_PATH = os.path.join(DATASET_PATH, \"train\")\nTEST_PATH = os.path.join(DATASET_PATH, \"test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:01:06.331330Z","iopub.execute_input":"2025-03-31T16:01:06.331636Z","iopub.status.idle":"2025-03-31T16:01:06.335533Z","shell.execute_reply.started":"2025-03-31T16:01:06.331610Z","shell.execute_reply":"2025-03-31T16:01:06.334637Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"CLASS_NAMES = [\"bg\", \"face\", \"lb\", \"rb\", \"le\", \"re\", \"nose\", \"ulip\", \"imouth\", \"llip\", \"hair\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:01:07.180184Z","iopub.execute_input":"2025-03-31T16:01:07.180464Z","iopub.status.idle":"2025-03-31T16:01:07.184245Z","shell.execute_reply.started":"2025-03-31T16:01:07.180444Z","shell.execute_reply":"2025-03-31T16:01:07.183360Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class HelenDataset(Dataset):\n    def __init__(self, data_path, transform=None):\n        self.data_path = data_path\n        self.transform = transform\n        self.images_path = []\n        self.masks_path = []\n        \n        # Get all image files\n        image_files = glob(os.path.join(data_path, \"*_image.jpg\"))\n        \n        for img_path in image_files:\n            base_name = os.path.basename(img_path).replace(\"_image.jpg\", \"\")\n            mask_path = os.path.join(data_path, f\"{base_name}_label.png\")\n            \n            if os.path.exists(mask_path):\n                self.images_path.append(img_path)\n                self.masks_path.append(mask_path)\n    \n    def __len__(self):\n        return len(self.images_path)\n    \n    def __getitem__(self, idx):\n        img_path = self.images_path[idx]\n        mask_path = self.masks_path[idx]\n        \n        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        image = np.stack((image,)*3, axis=-1)  # Convert to 3 channels\n        \n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        \n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image = augmented[\"image\"]\n            mask = augmented[\"mask\"]\n        \n        image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n        mask = torch.from_numpy(mask).long()\n        \n        return image, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:01:08.004813Z","iopub.execute_input":"2025-03-31T16:01:08.005113Z","iopub.status.idle":"2025-03-31T16:01:08.011963Z","shell.execute_reply.started":"2025-03-31T16:01:08.005089Z","shell.execute_reply":"2025-03-31T16:01:08.011073Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return self.conv(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:01:09.400119Z","iopub.execute_input":"2025-03-31T16:01:09.400406Z","iopub.status.idle":"2025-03-31T16:01:09.405213Z","shell.execute_reply.started":"2025-03-31T16:01:09.400385Z","shell.execute_reply":"2025-03-31T16:01:09.404497Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=NUM_CLASSES, features=[64, 128, 256, 512]):\n        super(UNet, self).__init__()\n        self.downs = nn.ModuleList()\n        self.ups = nn.ModuleList()\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        # Downsampling (encoder)\n        for feature in features:\n            self.downs.append(DoubleConv(in_channels, feature))\n            in_channels = feature\n        \n        # Bottleneck\n        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n        \n        # Upsampling (decoder)\n        for feature in reversed(features):\n            self.ups.append(\n                nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2)\n            )\n            self.ups.append(DoubleConv(feature*2, feature))\n        \n        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n    \n    def forward(self, x):\n        skip_connections = []\n        \n        \n        for down in self.downs:\n            x = down(x)\n            skip_connections.append(x)\n            x = self.pool(x)\n        \n        x = self.bottleneck(x)\n        skip_connections = skip_connections[::-1]  # Reverse for decoder\n        \n        for idx in range(0, len(self.ups), 2):\n            x = self.ups[idx](x)  # ConvTranspose2d\n            skip_connection = skip_connections[idx//2]\n            \n            if x.shape != skip_connection.shape:\n                x = TF.resize(x, size=skip_connection.shape[2:])\n                \n            concat_skip = torch.cat((skip_connection, x), dim=1)\n            x = self.ups[idx+1](concat_skip)  # DoubleConv\n        \n        return self.final_conv(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:01:10.321019Z","iopub.execute_input":"2025-03-31T16:01:10.321305Z","iopub.status.idle":"2025-03-31T16:01:10.328818Z","shell.execute_reply.started":"2025-03-31T16:01:10.321284Z","shell.execute_reply":"2025-03-31T16:01:10.327916Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"class Augmentation:\n    def __init__(self):\n        pass\n    \n    def __call__(self, image, mask):\n        image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n        mask = cv2.resize(mask, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_NEAREST)\n        \n        if random.random() > 0.5:\n            image = np.fliplr(image).copy()\n            mask = np.fliplr(mask).copy()\n        \n        if random.random() > 0.5:\n            angle = random.randint(-15, 15)\n            M = cv2.getRotationMatrix2D((IMAGE_SIZE/2, IMAGE_SIZE/2), angle, 1)\n            image = cv2.warpAffine(image, M, (IMAGE_SIZE, IMAGE_SIZE))\n            mask = cv2.warpAffine(mask, M, (IMAGE_SIZE, IMAGE_SIZE), flags=cv2.INTER_NEAREST)\n        \n        return {\"image\": image, \"mask\": mask}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:01:11.960789Z","iopub.execute_input":"2025-03-31T16:01:11.961116Z","iopub.status.idle":"2025-03-31T16:01:11.966443Z","shell.execute_reply.started":"2025-03-31T16:01:11.961092Z","shell.execute_reply":"2025-03-31T16:01:11.965726Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def train_fn(model, train_loader, optimizer, criterion, device):\n    model.train()\n    epoch_loss = 0\n    \n    for batch_idx, (data, targets) in enumerate(train_loader):\n        data = data.to(device)\n        targets = targets.to(device)\n        \n        predictions = model(data)\n        loss = criterion(predictions, targets)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        \n        if batch_idx % 10 == 0:\n            print(f\"Batch {batch_idx}/{len(train_loader)}: Loss = {loss.item():.4f}\")\n    \n    return epoch_loss / len(train_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:01:13.986836Z","iopub.execute_input":"2025-03-31T16:01:13.987162Z","iopub.status.idle":"2025-03-31T16:01:13.992027Z","shell.execute_reply.started":"2025-03-31T16:01:13.987138Z","shell.execute_reply":"2025-03-31T16:01:13.991236Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def eval_fn(model, val_loader, criterion, device):\n    model.eval()\n    epoch_loss = 0\n    \n    with torch.no_grad():\n        for data, targets in val_loader:\n            data = data.to(device)\n            targets = targets.to(device)\n            \n            predictions = model(data)\n            loss = criterion(predictions, targets)\n            \n            epoch_loss += loss.item()\n    \n    return epoch_loss / len(val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:01:14.929238Z","iopub.execute_input":"2025-03-31T16:01:14.929551Z","iopub.status.idle":"2025-03-31T16:01:14.934475Z","shell.execute_reply.started":"2025-03-31T16:01:14.929523Z","shell.execute_reply":"2025-03-31T16:01:14.933608Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def calculate_metrics(model, data_loader, device, num_classes=NUM_CLASSES):\n    model.eval()\n    \n    confusion_matrix = np.zeros((num_classes, num_classes))\n    \n    with torch.no_grad():\n        for data, targets in data_loader:\n            data = data.to(device)\n            targets = targets.to(device)\n            \n            outputs = model(data)\n            _, predictions = torch.max(outputs, dim=1)\n            \n            for i in range(targets.shape[0]):\n                for t, p in zip(targets[i].flatten(), predictions[i].flatten()):\n                    confusion_matrix[t.item(), p.item()] += 1\n    \n    ious = []\n    f1_scores = []\n    \n    for i in range(num_classes):\n        tp = confusion_matrix[i, i]\n        fp = confusion_matrix[:, i].sum() - tp\n        fn = confusion_matrix[i, :].sum() - tp\n        \n        iou = tp / (tp + fp + fn + 1e-10)\n        ious.append(iou)\n        \n        # Calculate F1 score\n        precision = tp / (tp + fp + 1e-10)\n        recall = tp / (tp + fn + 1e-10)\n        f1 = 2 * precision * recall / (precision + recall + 1e-10)\n        f1_scores.append(f1)\n    \n    return ious, f1_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:01:15.900777Z","iopub.execute_input":"2025-03-31T16:01:15.901104Z","iopub.status.idle":"2025-03-31T16:01:15.907368Z","shell.execute_reply.started":"2025-03-31T16:01:15.901078Z","shell.execute_reply":"2025-03-31T16:01:15.906459Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def visualize_predictions(model, data_loader, device, num_samples=3):\n    model.eval()\n    \n    samples = []\n    counter = 0\n    \n    with torch.no_grad():\n        for data, targets in data_loader:\n            if counter >= num_samples:\n                break\n            \n            data = data.to(device)\n            \n            outputs = model(data)\n            _, predictions = torch.max(outputs, dim=1)\n            \n            data = data.cpu()\n            targets = targets.cpu()\n            predictions = predictions.cpu()\n            \n            for i in range(data.shape[0]):\n                if counter >= num_samples:\n                    break\n                \n                samples.append((data[i], targets[i], predictions[i]))\n                counter += 1\n    \n    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\n    \n    for i, (image, target, prediction) in enumerate(samples):\n        img = image.permute(1, 2, 0).numpy()\n        axes[i, 0].imshow(img[:,:,0], cmap='gray')\n        axes[i, 0].set_title(\"Original Image\")\n        axes[i, 0].axis('off')\n        \n        axes[i, 1].imshow(target, cmap='nipy_spectral', vmin=0, vmax=NUM_CLASSES-1)\n        axes[i, 1].set_title(\"Ground Truth\")\n        axes[i, 1].axis('off')\n        \n        axes[i, 2].imshow(prediction, cmap='nipy_spectral', vmin=0, vmax=NUM_CLASSES-1)\n        axes[i, 2].set_title(\"Prediction\")\n        axes[i, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(\"prediction_samples.png\")\n    plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:01:16.912664Z","iopub.execute_input":"2025-03-31T16:01:16.912990Z","iopub.status.idle":"2025-03-31T16:01:16.920797Z","shell.execute_reply.started":"2025-03-31T16:01:16.912964Z","shell.execute_reply":"2025-03-31T16:01:16.919755Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def create_visualization_mask(pred_mask, output_path):\n    colors = [\n        [0, 0, 0],       # bg - black\n        [255, 0, 0],     # face - red\n        [0, 255, 0],     # lb - green\n        [0, 0, 255],     # rb - blue\n        [255, 255, 0],   # le - yellow\n        [255, 0, 255],   # re - magenta\n        [0, 255, 255],   # nose - cyan\n        [128, 0, 0],     # ulip - maroon\n        [0, 128, 0],     # imouth - dark green\n        [0, 0, 128],     # llip - navy\n        [128, 128, 0]    # hair - olive\n    ]\n    \n    h, w = pred_mask.shape\n    viz_mask = np.zeros((h, w, 3), dtype=np.uint8)\n    \n    for i in range(NUM_CLASSES):\n        viz_mask[pred_mask == i] = colors[i]\n    \n    cv2.imwrite(output_path, cv2.cvtColor(viz_mask, cv2.COLOR_RGB2BGR))\n    \n    return viz_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:01:18.990721Z","iopub.execute_input":"2025-03-31T16:01:18.991047Z","iopub.status.idle":"2025-03-31T16:01:18.996417Z","shell.execute_reply.started":"2025-03-31T16:01:18.991021Z","shell.execute_reply":"2025-03-31T16:01:18.995731Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def predict_on_image(model, image_path, device):\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n    img = np.stack((img,)*3, axis=-1)  # Convert to 3 channels\n    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n    \n    img_tensor = torch.from_numpy(img.transpose(2, 0, 1)).float() / 255.0\n    img_tensor = img_tensor.unsqueeze(0).to(device)  # Add batch dimension\n    \n    model.eval()\n    with torch.no_grad():\n        output = model(img_tensor)\n        mask = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n    \n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.imshow(img[:,:,0], cmap='gray')\n    plt.title(\"Input Image\")\n    plt.axis('off')\n    \n    plt.subplot(1, 2, 2)\n    plt.imshow(mask, cmap='nipy_spectral', vmin=0, vmax=NUM_CLASSES-1)\n    plt.title(\"Predicted Segmentation\")\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(\"new_prediction.png\")\n    plt.close()\n    \n    viz_mask = create_visualization_mask(mask, \"colorized_prediction.png\")\n    \n    return mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:01:20.785956Z","iopub.execute_input":"2025-03-31T16:01:20.786239Z","iopub.status.idle":"2025-03-31T16:01:20.792388Z","shell.execute_reply.started":"2025-03-31T16:01:20.786218Z","shell.execute_reply":"2025-03-31T16:01:20.791572Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:01:21.882843Z","iopub.execute_input":"2025-03-31T16:01:21.883174Z","iopub.status.idle":"2025-03-31T16:01:22.034131Z","shell.execute_reply.started":"2025-03-31T16:01:21.883150Z","shell.execute_reply":"2025-03-31T16:01:22.033443Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def main():\n    train_transform = Augmentation()\n    val_transform = Augmentation()  # No augmentation for validation\n    \n    train_dataset = HelenDataset(TRAIN_PATH, transform=train_transform)\n    \n    train_indices, val_indices = train_test_split(\n        range(len(train_dataset)),\n        test_size=0.2,\n        random_state=42\n    )\n    \n    train_subset = torch.utils.data.Subset(train_dataset, train_indices)\n    val_subset = torch.utils.data.Subset(train_dataset, val_indices)\n    \n    test_dataset = HelenDataset(TEST_PATH, transform=val_transform)\n    \n    train_loader = DataLoader(\n        train_subset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        num_workers=4,\n        pin_memory=True\n    )\n    \n    val_loader = DataLoader(\n        val_subset,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        num_workers=4,\n        pin_memory=True\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        num_workers=4,\n        pin_memory=True\n    )\n    \n    print(f\"Training samples: {len(train_subset)}\")\n    print(f\"Validation samples: {len(val_subset)}\")\n    print(f\"Test samples: {len(test_dataset)}\")\n    \n    model = UNet().to(DEVICE)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer,\n        mode='min',\n        factor=0.1,\n        patience=5,\n        verbose=True\n    )\n    \n    best_val_loss = float('inf')\n    train_losses = []\n    val_losses = []\n    \n    print(f\"Starting training on device: {DEVICE}\")\n    \n    for epoch in range(EPOCHS):\n        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n        \n        train_loss = train_fn(model, train_loader, optimizer, criterion, DEVICE)\n        \n        val_loss = eval_fn(model, val_loader, criterion, DEVICE)\n        \n        scheduler.step(val_loss)\n        \n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        \n        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), \"best_model.pth\")\n            print(\"Saved best model!\")\n    \n    model.load_state_dict(torch.load(\"best_model.pth\"))\n    \n    plt.figure(figsize=(10, 5))\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(val_losses, label='Val Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.savefig(\"training_history.png\")\n    plt.close()\n    \n    print(\"Calculating metrics on test set...\")\n    ious, f1_scores = calculate_metrics(model, test_loader, DEVICE)\n    \n    print(\"\\nPer-class metrics:\")\n    print(\"------------------\")\n    \n    for i in range(NUM_CLASSES):\n        print(f\"Class {i} ({CLASS_NAMES[i]}):\")\n        print(f\"  IoU: {ious[i]:.4f}\")\n        print(f\"  F1 Score: {f1_scores[i]:.4f}\")\n    \n    mean_iou = np.mean(ious)\n    mean_f1 = np.mean(f1_scores)\n    \n    print(\"\\nOverall metrics:\")\n    print(\"----------------\")\n    print(f\"Mean IoU: {mean_iou:.4f}\")\n    print(f\"Mean F1 Score: {mean_f1:.4f}\")\n    \n    print(\"Generating prediction visualizations...\")\n    visualize_predictions(model, test_loader, DEVICE)\n    \n    print(\"Training and evaluation complete!\")\n    print(\"Trained model saved as: best_model.pth\")\n    print(\"Training history plot saved as: training_history.png\")\n    print(\"Prediction samples saved as: prediction_samples.png\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T16:01:23.133783Z","iopub.execute_input":"2025-03-31T16:01:23.134119Z","iopub.status.idle":"2025-03-31T16:43:21.768255Z","shell.execute_reply.started":"2025-03-31T16:01:23.134092Z","shell.execute_reply":"2025-03-31T16:43:21.767161Z"}},"outputs":[{"name":"stdout","text":"Training samples: 1599\nValidation samples: 400\nTest samples: 100\nStarting training on device: cuda\nEpoch 1/10\nBatch 0/400: Loss = 2.6170\nBatch 10/400: Loss = 2.4566\nBatch 20/400: Loss = 2.2720\nBatch 30/400: Loss = 2.1051\nBatch 40/400: Loss = 2.0392\nBatch 50/400: Loss = 1.8862\nBatch 60/400: Loss = 1.8740\nBatch 70/400: Loss = 1.8732\nBatch 80/400: Loss = 1.8154\nBatch 90/400: Loss = 1.7794\nBatch 100/400: Loss = 1.7641\nBatch 110/400: Loss = 1.7637\nBatch 120/400: Loss = 1.6827\nBatch 130/400: Loss = 1.7167\nBatch 140/400: Loss = 1.5849\nBatch 150/400: Loss = 1.6691\nBatch 160/400: Loss = 1.5847\nBatch 170/400: Loss = 1.4836\nBatch 180/400: Loss = 1.5329\nBatch 190/400: Loss = 1.4410\nBatch 200/400: Loss = 1.4458\nBatch 210/400: Loss = 1.3357\nBatch 220/400: Loss = 1.3806\nBatch 230/400: Loss = 1.5001\nBatch 240/400: Loss = 1.3326\nBatch 250/400: Loss = 1.3940\nBatch 260/400: Loss = 1.3125\nBatch 270/400: Loss = 1.2669\nBatch 280/400: Loss = 1.2646\nBatch 290/400: Loss = 1.1549\nBatch 300/400: Loss = 1.0789\nBatch 310/400: Loss = 1.1334\nBatch 320/400: Loss = 1.3792\nBatch 330/400: Loss = 1.2583\nBatch 340/400: Loss = 0.9648\nBatch 350/400: Loss = 1.1181\nBatch 360/400: Loss = 1.4999\nBatch 370/400: Loss = 1.1296\nBatch 380/400: Loss = 1.0668\nBatch 390/400: Loss = 1.1171\nTrain Loss: 1.5240, Val Loss: 1.2539\nSaved best model!\nEpoch 2/10\nBatch 0/400: Loss = 1.0113\nBatch 10/400: Loss = 1.1245\nBatch 20/400: Loss = 0.9634\nBatch 30/400: Loss = 1.0782\nBatch 40/400: Loss = 0.9343\nBatch 50/400: Loss = 0.9948\nBatch 60/400: Loss = 1.0883\nBatch 70/400: Loss = 0.9012\nBatch 80/400: Loss = 0.9689\nBatch 90/400: Loss = 0.8231\nBatch 100/400: Loss = 0.8225\nBatch 110/400: Loss = 0.8638\nBatch 120/400: Loss = 0.7982\nBatch 130/400: Loss = 0.6864\nBatch 140/400: Loss = 0.7693\nBatch 150/400: Loss = 0.8373\nBatch 160/400: Loss = 0.8273\nBatch 170/400: Loss = 0.7982\nBatch 180/400: Loss = 0.8560\nBatch 190/400: Loss = 0.6413\nBatch 200/400: Loss = 0.7659\nBatch 210/400: Loss = 1.1330\nBatch 220/400: Loss = 0.7907\nBatch 230/400: Loss = 0.6973\nBatch 240/400: Loss = 0.7897\nBatch 250/400: Loss = 0.6993\nBatch 260/400: Loss = 0.6671\nBatch 270/400: Loss = 0.6139\nBatch 280/400: Loss = 0.7691\nBatch 290/400: Loss = 0.6750\nBatch 300/400: Loss = 0.7580\nBatch 310/400: Loss = 0.5652\nBatch 320/400: Loss = 0.8417\nBatch 330/400: Loss = 0.6198\nBatch 340/400: Loss = 0.7001\nBatch 350/400: Loss = 0.6273\nBatch 360/400: Loss = 0.6123\nBatch 370/400: Loss = 0.7271\nBatch 380/400: Loss = 0.6306\nBatch 390/400: Loss = 0.6570\nTrain Loss: 0.8019, Val Loss: 0.5856\nSaved best model!\nEpoch 3/10\nBatch 0/400: Loss = 0.6315\nBatch 10/400: Loss = 0.7420\nBatch 20/400: Loss = 0.5473\nBatch 30/400: Loss = 0.5292\nBatch 40/400: Loss = 0.5515\nBatch 50/400: Loss = 0.4862\nBatch 60/400: Loss = 0.4715\nBatch 70/400: Loss = 0.5392\nBatch 80/400: Loss = 0.4425\nBatch 90/400: Loss = 0.5681\nBatch 100/400: Loss = 0.5321\nBatch 110/400: Loss = 0.6171\nBatch 120/400: Loss = 0.5529\nBatch 130/400: Loss = 0.5080\nBatch 140/400: Loss = 0.5678\nBatch 150/400: Loss = 0.4589\nBatch 160/400: Loss = 0.4753\nBatch 170/400: Loss = 0.5431\nBatch 180/400: Loss = 0.6253\nBatch 190/400: Loss = 0.7927\nBatch 200/400: Loss = 0.5372\nBatch 210/400: Loss = 0.5256\nBatch 220/400: Loss = 0.5832\nBatch 230/400: Loss = 0.4957\nBatch 240/400: Loss = 0.4981\nBatch 250/400: Loss = 0.5985\nBatch 260/400: Loss = 0.4876\nBatch 270/400: Loss = 0.5772\nBatch 280/400: Loss = 0.5006\nBatch 290/400: Loss = 0.5750\nBatch 300/400: Loss = 0.5118\nBatch 310/400: Loss = 0.3664\nBatch 320/400: Loss = 0.4714\nBatch 330/400: Loss = 0.5213\nBatch 340/400: Loss = 0.3825\nBatch 350/400: Loss = 0.4917\nBatch 360/400: Loss = 0.4245\nBatch 370/400: Loss = 0.4971\nBatch 380/400: Loss = 0.6221\nBatch 390/400: Loss = 0.5932\nTrain Loss: 0.5649, Val Loss: 0.4939\nSaved best model!\nEpoch 4/10\nBatch 0/400: Loss = 0.5351\nBatch 10/400: Loss = 0.6845\nBatch 20/400: Loss = 0.4049\nBatch 30/400: Loss = 0.4079\nBatch 40/400: Loss = 0.4930\nBatch 50/400: Loss = 0.3992\nBatch 60/400: Loss = 0.6312\nBatch 70/400: Loss = 0.6309\nBatch 80/400: Loss = 0.4255\nBatch 90/400: Loss = 0.3369\nBatch 100/400: Loss = 0.9431\nBatch 110/400: Loss = 0.5673\nBatch 120/400: Loss = 0.6119\nBatch 130/400: Loss = 0.8290\nBatch 140/400: Loss = 0.4846\nBatch 150/400: Loss = 0.3515\nBatch 160/400: Loss = 0.4325\nBatch 170/400: Loss = 0.4000\nBatch 180/400: Loss = 0.6308\nBatch 190/400: Loss = 0.4475\nBatch 200/400: Loss = 0.5210\nBatch 210/400: Loss = 0.7692\nBatch 220/400: Loss = 0.3926\nBatch 230/400: Loss = 0.3531\nBatch 240/400: Loss = 0.3906\nBatch 250/400: Loss = 0.5415\nBatch 260/400: Loss = 0.4502\nBatch 270/400: Loss = 0.4155\nBatch 280/400: Loss = 0.3484\nBatch 290/400: Loss = 0.3703\nBatch 300/400: Loss = 0.4361\nBatch 310/400: Loss = 0.5199\nBatch 320/400: Loss = 0.5021\nBatch 330/400: Loss = 0.3640\nBatch 340/400: Loss = 0.3640\nBatch 350/400: Loss = 0.3890\nBatch 360/400: Loss = 0.6266\nBatch 370/400: Loss = 0.3836\nBatch 380/400: Loss = 0.6609\nBatch 390/400: Loss = 0.5406\nTrain Loss: 0.4605, Val Loss: 0.4046\nSaved best model!\nEpoch 5/10\nBatch 0/400: Loss = 0.5563\nBatch 10/400: Loss = 0.4283\nBatch 20/400: Loss = 0.4923\nBatch 30/400: Loss = 0.2882\nBatch 40/400: Loss = 0.3733\nBatch 50/400: Loss = 0.3245\nBatch 60/400: Loss = 0.5510\nBatch 70/400: Loss = 0.4679\nBatch 80/400: Loss = 0.3429\nBatch 90/400: Loss = 0.3449\nBatch 100/400: Loss = 0.3832\nBatch 110/400: Loss = 0.5712\nBatch 120/400: Loss = 0.3828\nBatch 130/400: Loss = 0.3946\nBatch 140/400: Loss = 0.2707\nBatch 150/400: Loss = 0.5196\nBatch 160/400: Loss = 0.4617\nBatch 170/400: Loss = 0.6967\nBatch 180/400: Loss = 0.2576\nBatch 190/400: Loss = 0.4000\nBatch 200/400: Loss = 0.4699\nBatch 210/400: Loss = 0.3441\nBatch 220/400: Loss = 0.5101\nBatch 230/400: Loss = 0.4882\nBatch 240/400: Loss = 0.4161\nBatch 250/400: Loss = 0.3539\nBatch 260/400: Loss = 0.2998\nBatch 270/400: Loss = 0.3479\nBatch 280/400: Loss = 0.3774\nBatch 290/400: Loss = 0.3005\nBatch 300/400: Loss = 0.2656\nBatch 310/400: Loss = 0.2958\nBatch 320/400: Loss = 0.4167\nBatch 330/400: Loss = 0.2697\nBatch 340/400: Loss = 0.5596\nBatch 350/400: Loss = 0.3488\nBatch 360/400: Loss = 0.3005\nBatch 370/400: Loss = 0.4053\nBatch 380/400: Loss = 0.4524\nBatch 390/400: Loss = 0.2825\nTrain Loss: 0.3957, Val Loss: 0.3436\nSaved best model!\nEpoch 6/10\nBatch 0/400: Loss = 0.3144\nBatch 10/400: Loss = 0.3193\nBatch 20/400: Loss = 0.5097\nBatch 30/400: Loss = 0.4605\nBatch 40/400: Loss = 0.3255\nBatch 50/400: Loss = 0.3615\nBatch 60/400: Loss = 0.5505\nBatch 70/400: Loss = 0.3313\nBatch 80/400: Loss = 0.2868\nBatch 90/400: Loss = 0.4651\nBatch 100/400: Loss = 0.5856\nBatch 110/400: Loss = 0.2964\nBatch 120/400: Loss = 0.5437\nBatch 130/400: Loss = 0.3078\nBatch 140/400: Loss = 0.2853\nBatch 150/400: Loss = 0.3947\nBatch 160/400: Loss = 0.3892\nBatch 170/400: Loss = 0.2662\nBatch 180/400: Loss = 0.3594\nBatch 190/400: Loss = 0.4106\nBatch 200/400: Loss = 0.2758\nBatch 210/400: Loss = 0.3425\nBatch 220/400: Loss = 0.4309\nBatch 230/400: Loss = 0.2184\nBatch 240/400: Loss = 0.5802\nBatch 250/400: Loss = 0.2719\nBatch 260/400: Loss = 0.5171\nBatch 270/400: Loss = 0.4642\nBatch 280/400: Loss = 0.3133\nBatch 290/400: Loss = 0.2461\nBatch 300/400: Loss = 0.5572\nBatch 310/400: Loss = 0.4452\nBatch 320/400: Loss = 0.3141\nBatch 330/400: Loss = 0.3030\nBatch 340/400: Loss = 0.3366\nBatch 350/400: Loss = 0.2862\nBatch 360/400: Loss = 0.2811\nBatch 370/400: Loss = 0.2445\nBatch 380/400: Loss = 0.3094\nBatch 390/400: Loss = 0.4110\nTrain Loss: 0.3497, Val Loss: 0.3315\nSaved best model!\nEpoch 7/10\nBatch 0/400: Loss = 0.2456\nBatch 10/400: Loss = 0.2598\nBatch 20/400: Loss = 0.3103\nBatch 30/400: Loss = 0.2276\nBatch 40/400: Loss = 0.2157\nBatch 50/400: Loss = 0.2844\nBatch 60/400: Loss = 0.2382\nBatch 70/400: Loss = 0.3583\nBatch 80/400: Loss = 0.2989\nBatch 90/400: Loss = 0.1597\nBatch 100/400: Loss = 0.2925\nBatch 110/400: Loss = 0.3388\nBatch 120/400: Loss = 0.2752\nBatch 130/400: Loss = 0.1689\nBatch 140/400: Loss = 0.3714\nBatch 150/400: Loss = 0.2634\nBatch 160/400: Loss = 0.2672\nBatch 170/400: Loss = 0.3047\nBatch 180/400: Loss = 0.3294\nBatch 190/400: Loss = 0.2879\nBatch 200/400: Loss = 0.3389\nBatch 210/400: Loss = 0.4330\nBatch 220/400: Loss = 0.2372\nBatch 230/400: Loss = 0.2487\nBatch 240/400: Loss = 0.2999\nBatch 250/400: Loss = 0.4447\nBatch 260/400: Loss = 0.3728\nBatch 270/400: Loss = 0.2646\nBatch 280/400: Loss = 0.2817\nBatch 290/400: Loss = 0.2283\nBatch 300/400: Loss = 0.2935\nBatch 310/400: Loss = 0.3972\nBatch 320/400: Loss = 0.5224\nBatch 330/400: Loss = 0.4268\nBatch 340/400: Loss = 0.3742\nBatch 350/400: Loss = 0.3155\nBatch 360/400: Loss = 0.3371\nBatch 370/400: Loss = 0.1626\nBatch 380/400: Loss = 0.2485\nBatch 390/400: Loss = 0.3386\nTrain Loss: 0.3185, Val Loss: 0.2893\nSaved best model!\nEpoch 8/10\nBatch 0/400: Loss = 0.2780\nBatch 10/400: Loss = 0.3441\nBatch 20/400: Loss = 0.3664\nBatch 30/400: Loss = 0.1955\nBatch 40/400: Loss = 0.2666\nBatch 50/400: Loss = 0.1861\nBatch 60/400: Loss = 0.2212\nBatch 70/400: Loss = 0.3623\nBatch 80/400: Loss = 0.3392\nBatch 90/400: Loss = 0.3203\nBatch 100/400: Loss = 0.2734\nBatch 110/400: Loss = 0.3841\nBatch 120/400: Loss = 0.3959\nBatch 130/400: Loss = 0.2472\nBatch 140/400: Loss = 0.1884\nBatch 150/400: Loss = 0.2466\nBatch 160/400: Loss = 0.3382\nBatch 170/400: Loss = 0.2007\nBatch 180/400: Loss = 0.2065\nBatch 190/400: Loss = 0.3552\nBatch 200/400: Loss = 0.2940\nBatch 210/400: Loss = 0.1595\nBatch 220/400: Loss = 0.4062\nBatch 230/400: Loss = 0.2093\nBatch 240/400: Loss = 0.1818\nBatch 250/400: Loss = 0.2874\nBatch 260/400: Loss = 0.3807\nBatch 270/400: Loss = 0.1548\nBatch 280/400: Loss = 0.2764\nBatch 290/400: Loss = 0.4108\nBatch 300/400: Loss = 0.2864\nBatch 310/400: Loss = 0.1910\nBatch 320/400: Loss = 0.4129\nBatch 330/400: Loss = 0.1890\nBatch 340/400: Loss = 0.7745\nBatch 350/400: Loss = 0.2764\nBatch 360/400: Loss = 0.2983\nBatch 370/400: Loss = 0.1642\nBatch 380/400: Loss = 0.4401\nBatch 390/400: Loss = 0.2086\nTrain Loss: 0.2939, Val Loss: 0.2771\nSaved best model!\nEpoch 9/10\nBatch 0/400: Loss = 0.3051\nBatch 10/400: Loss = 0.2709\nBatch 20/400: Loss = 0.1864\nBatch 30/400: Loss = 0.2159\nBatch 40/400: Loss = 0.2853\nBatch 50/400: Loss = 0.3284\nBatch 60/400: Loss = 0.3197\nBatch 70/400: Loss = 0.3354\nBatch 80/400: Loss = 0.2458\nBatch 90/400: Loss = 0.3076\nBatch 100/400: Loss = 0.1439\nBatch 110/400: Loss = 0.3285\nBatch 120/400: Loss = 0.3783\nBatch 130/400: Loss = 0.1787\nBatch 140/400: Loss = 0.3422\nBatch 150/400: Loss = 0.3556\nBatch 160/400: Loss = 0.3406\nBatch 170/400: Loss = 0.2806\nBatch 180/400: Loss = 0.2415\nBatch 190/400: Loss = 0.1766\nBatch 200/400: Loss = 0.6879\nBatch 210/400: Loss = 0.3086\nBatch 220/400: Loss = 0.2128\nBatch 230/400: Loss = 0.2041\nBatch 240/400: Loss = 0.2723\nBatch 250/400: Loss = 0.5783\nBatch 260/400: Loss = 0.2332\nBatch 270/400: Loss = 0.1661\nBatch 280/400: Loss = 0.2813\nBatch 290/400: Loss = 0.2619\nBatch 300/400: Loss = 0.2288\nBatch 310/400: Loss = 0.3402\nBatch 320/400: Loss = 0.1802\nBatch 330/400: Loss = 0.1872\nBatch 340/400: Loss = 0.3896\nBatch 350/400: Loss = 0.4474\nBatch 360/400: Loss = 0.2177\nBatch 370/400: Loss = 0.2197\nBatch 380/400: Loss = 0.2466\nBatch 390/400: Loss = 0.2904\nTrain Loss: 0.2850, Val Loss: 0.2595\nSaved best model!\nEpoch 10/10\nBatch 0/400: Loss = 0.3733\nBatch 10/400: Loss = 0.2140\nBatch 20/400: Loss = 0.3416\nBatch 30/400: Loss = 0.2555\nBatch 40/400: Loss = 0.4013\nBatch 50/400: Loss = 0.2100\nBatch 60/400: Loss = 0.2655\nBatch 70/400: Loss = 0.2287\nBatch 80/400: Loss = 0.1924\nBatch 90/400: Loss = 0.2666\nBatch 100/400: Loss = 0.1854\nBatch 110/400: Loss = 0.2653\nBatch 120/400: Loss = 0.2047\nBatch 130/400: Loss = 0.2512\nBatch 140/400: Loss = 0.2671\nBatch 150/400: Loss = 0.2512\nBatch 160/400: Loss = 0.2578\nBatch 170/400: Loss = 0.2859\nBatch 180/400: Loss = 0.1930\nBatch 190/400: Loss = 0.2239\nBatch 200/400: Loss = 0.2124\nBatch 210/400: Loss = 0.2675\nBatch 220/400: Loss = 0.3261\nBatch 230/400: Loss = 0.2922\nBatch 240/400: Loss = 0.1598\nBatch 250/400: Loss = 0.1862\nBatch 260/400: Loss = 0.3916\nBatch 270/400: Loss = 0.2565\nBatch 280/400: Loss = 0.2222\nBatch 290/400: Loss = 0.1524\nBatch 300/400: Loss = 0.3805\nBatch 310/400: Loss = 0.2117\nBatch 320/400: Loss = 0.1768\nBatch 330/400: Loss = 0.1941\nBatch 340/400: Loss = 0.2573\nBatch 350/400: Loss = 0.2237\nBatch 360/400: Loss = 0.1611\nBatch 370/400: Loss = 0.2698\nBatch 380/400: Loss = 0.3579\nBatch 390/400: Loss = 0.3385\nTrain Loss: 0.2664, Val Loss: 0.2594\nSaved best model!\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-34-71f1fa6f2f5c>:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"best_model.pth\"))\n","output_type":"stream"},{"name":"stdout","text":"Calculating metrics on test set...\n\nPer-class metrics:\n------------------\nClass 0 (bg):\n  IoU: 0.8903\n  F1 Score: 0.9420\nClass 1 (face):\n  IoU: 0.7718\n  F1 Score: 0.8712\nClass 2 (lb):\n  IoU: 0.1979\n  F1 Score: 0.3305\nClass 3 (rb):\n  IoU: 0.2126\n  F1 Score: 0.3507\nClass 4 (le):\n  IoU: 0.2570\n  F1 Score: 0.4090\nClass 5 (re):\n  IoU: 0.2208\n  F1 Score: 0.3618\nClass 6 (nose):\n  IoU: 0.7769\n  F1 Score: 0.8744\nClass 7 (ulip):\n  IoU: 0.4533\n  F1 Score: 0.6238\nClass 8 (imouth):\n  IoU: 0.5382\n  F1 Score: 0.6998\nClass 9 (llip):\n  IoU: 0.4511\n  F1 Score: 0.6217\nClass 10 (hair):\n  IoU: 0.6310\n  F1 Score: 0.7737\n\nOverall metrics:\n----------------\nMean IoU: 0.4910\nMean F1 Score: 0.6235\nGenerating prediction visualizations...\nTraining and evaluation complete!\nTrained model saved as: best_model.pth\nTraining history plot saved as: training_history.png\nPrediction samples saved as: prediction_samples.png\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}