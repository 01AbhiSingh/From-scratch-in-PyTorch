{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7121520,"sourceType":"datasetVersion","datasetId":4107678}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-01T03:50:39.278758Z","iopub.execute_input":"2025-06-01T03:50:39.278955Z","iopub.status.idle":"2025-06-01T03:50:39.546381Z","shell.execute_reply.started":"2025-06-01T03:50:39.278938Z","shell.execute_reply":"2025-06-01T03:50:39.545675Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install transformers -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T03:50:39.547192Z","iopub.execute_input":"2025-06-01T03:50:39.547535Z","iopub.status.idle":"2025-06-01T03:50:44.021573Z","shell.execute_reply.started":"2025-06-01T03:50:39.547507Z","shell.execute_reply":"2025-06-01T03:50:44.020780Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import BertForSequenceClassification\nfrom transformers import AutoTokenizer\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T03:50:44.023991Z","iopub.execute_input":"2025-06-01T03:50:44.024547Z","iopub.status.idle":"2025-06-01T03:51:11.545581Z","shell.execute_reply.started":"2025-06-01T03:50:44.024505Z","shell.execute_reply":"2025-06-01T03:51:11.545005Z"}},"outputs":[{"name":"stderr","text":"2025-06-01 03:50:58.952499: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748749859.176406      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748749859.241361      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/text-document-classification-dataset/df_file.csv')\nprint(\"Data shape:\", data.shape)\nprint(\"Label distribution:\")\nprint(data['Label'].value_counts())\nprint(\"Unique labels:\", data['Label'].unique())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T03:53:31.544025Z","iopub.execute_input":"2025-06-01T03:53:31.544634Z","iopub.status.idle":"2025-06-01T03:53:31.611835Z","shell.execute_reply.started":"2025-06-01T03:53:31.544605Z","shell.execute_reply":"2025-06-01T03:53:31.611204Z"}},"outputs":[{"name":"stdout","text":"Data shape: (2225, 2)\nLabel distribution:\nLabel\n1    511\n4    510\n0    417\n2    401\n3    386\nName: count, dtype: int64\nUnique labels: [0 1 2 3 4]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"train_X, test_X, train_Y, test_Y = train_test_split(\n    data['Text'], \n    data['Label'],  \n    train_size=0.7, \n    random_state=42  \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T03:53:32.011180Z","iopub.execute_input":"2025-06-01T03:53:32.011769Z","iopub.status.idle":"2025-06-01T03:53:32.017950Z","shell.execute_reply.started":"2025-06-01T03:53:32.011748Z","shell.execute_reply":"2025-06-01T03:53:32.017008Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\ntrain_tokens = tokenizer(list(train_X), padding=True, truncation=True, max_length=512)\ntest_tokens = tokenizer(list(test_X), padding=True, truncation=True, max_length=512)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T03:53:33.229679Z","iopub.execute_input":"2025-06-01T03:53:33.229960Z","iopub.status.idle":"2025-06-01T03:53:34.609629Z","shell.execute_reply.started":"2025-06-01T03:53:33.229939Z","shell.execute_reply":"2025-06-01T03:53:34.608751Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class TokenData(Dataset):\n    def __init__(self, train=False): \n        if train:\n            self.text_data = train_X\n            self.tokens = train_tokens\n            self.labels = list(train_Y)\n        else:\n            self.text_data = test_X\n            self.tokens = test_tokens\n            self.labels = list(test_Y)\n    \n    def __len__(self):  \n        return len(self.text_data)\n    \n    def __getitem__(self, idx):  \n        sample = {}\n        for k, v in self.tokens.items():\n            sample[k] = torch.tensor(v[idx])\n        sample['Label'] = torch.tensor(self.labels[idx], dtype=torch.long)  \n        return sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T03:56:51.426658Z","iopub.execute_input":"2025-06-01T03:56:51.426977Z","iopub.status.idle":"2025-06-01T03:56:51.433325Z","shell.execute_reply.started":"2025-06-01T03:56:51.426954Z","shell.execute_reply":"2025-06-01T03:56:51.432256Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"batch_size = 40\ntrain_dataset = TokenData(train=True)\ntrain_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\ntest_dataset = TokenData(train=False)\ntest_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T03:56:52.601902Z","iopub.execute_input":"2025-06-01T03:56:52.602169Z","iopub.status.idle":"2025-06-01T03:56:52.606666Z","shell.execute_reply.started":"2025-06-01T03:56:52.602149Z","shell.execute_reply":"2025-06-01T03:56:52.606081Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"num_classes = len(data['Label'].unique())\nprint(f\"Number of classes in dataset: {num_classes}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T03:56:53.475361Z","iopub.execute_input":"2025-06-01T03:56:53.475672Z","iopub.status.idle":"2025-06-01T03:56:53.480799Z","shell.execute_reply.started":"2025-06-01T03:56:53.475649Z","shell.execute_reply":"2025-06-01T03:56:53.479882Z"}},"outputs":[{"name":"stdout","text":"Number of classes in dataset: 5\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"bert_model = BertForSequenceClassification.from_pretrained(\n    'bert-base-cased', \n    num_labels=num_classes\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T03:56:55.438392Z","iopub.execute_input":"2025-06-01T03:56:55.438676Z","iopub.status.idle":"2025-06-01T03:56:58.291183Z","shell.execute_reply.started":"2025-06-01T03:56:55.438656Z","shell.execute_reply":"2025-06-01T03:56:58.290465Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f841237113f497589e12f74c13653cc"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"optimizer = AdamW(bert_model.parameters(), lr=2e-5)\nloss_fn = torch.nn.CrossEntropyLoss()\n\nnum_epochs = 4\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\nbert_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T03:57:00.386351Z","iopub.execute_input":"2025-06-01T03:57:00.386757Z","iopub.status.idle":"2025-06-01T03:57:00.733097Z","shell.execute_reply.started":"2025-06-01T03:57:00.386735Z","shell.execute_reply":"2025-06-01T03:57:00.732488Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=5, bias=True)\n)"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    print(f\"Epoch: {epoch + 1}\")\n    \n    # Training\n    bert_model.train()\n    total_train_loss = 0\n    \n    for i, batch in enumerate(train_loader):\n        \n        batch = {k: v.to(device) for k, v in batch.items()}\n        \n      \n        optimizer.zero_grad()\n        \n       \n        outputs = bert_model(\n            input_ids=batch['input_ids'], \n            attention_mask=batch['attention_mask'],\n            labels=batch['Label']  \n        )\n        \n   \n        loss = outputs.loss\n        \n       \n        loss.backward()\n        optimizer.step()\n        \n       \n        total_train_loss += loss.item()\n        \n        if (i + 1) % 10 == 0:  # Print every 10 batches\n            avg_loss = total_train_loss / (i + 1)\n            print(f'Training batch {i + 1}, Average loss: {avg_loss:.4f}')\n    \n   \n    avg_epoch_loss = total_train_loss / len(train_loader)\n    print(f\"Training epoch {epoch + 1} average loss: {avg_epoch_loss:.4f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T03:57:15.366184Z","iopub.execute_input":"2025-06-01T03:57:15.366449Z","iopub.status.idle":"2025-06-01T04:07:21.153423Z","shell.execute_reply.started":"2025-06-01T03:57:15.366430Z","shell.execute_reply":"2025-06-01T04:07:21.152585Z"}},"outputs":[{"name":"stdout","text":"Epoch: 1\nTraining batch 10, Average loss: 1.5646\nTraining batch 20, Average loss: 1.3644\nTraining batch 30, Average loss: 1.1399\nTraining epoch 1 average loss: 0.9659\n\nEpoch: 2\nTraining batch 10, Average loss: 0.2291\nTraining batch 20, Average loss: 0.1876\nTraining batch 30, Average loss: 0.1649\nTraining epoch 2 average loss: 0.1528\n\nEpoch: 3\nTraining batch 10, Average loss: 0.0586\nTraining batch 20, Average loss: 0.0545\nTraining batch 30, Average loss: 0.0543\nTraining epoch 3 average loss: 0.0537\n\nEpoch: 4\nTraining batch 10, Average loss: 0.0305\nTraining batch 20, Average loss: 0.0264\nTraining batch 30, Average loss: 0.0257\nTraining epoch 4 average loss: 0.0242\n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(\"Starting validation...\")\nbert_model.eval()\ntotal_val_loss = 0\ncorrect_predictions = 0\ntotal_predictions = 0\n\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        \n        outputs = bert_model(\n            input_ids=batch['input_ids'],\n            attention_mask=batch['attention_mask'],\n            labels=batch['Label']\n        )\n        \n        total_val_loss += outputs.loss.item()\n        \n        # Calculate accuracy\n        predictions = torch.argmax(outputs.logits, dim=-1)\n        correct_predictions += (predictions == batch['Label']).sum().item()\n        total_predictions += batch['Label'].size(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T04:07:49.021027Z","iopub.execute_input":"2025-06-01T04:07:49.021286Z","iopub.status.idle":"2025-06-01T04:08:09.712230Z","shell.execute_reply.started":"2025-06-01T04:07:49.021267Z","shell.execute_reply":"2025-06-01T04:08:09.711670Z"}},"outputs":[{"name":"stdout","text":"Starting validation...\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"avg_val_loss = total_val_loss / len(test_loader)\naccuracy = correct_predictions / total_predictions\n\nprint(f\"Validation Loss: {avg_val_loss:.4f}\")\nprint(f\"Validation Accuracy: {accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T04:08:13.265414Z","iopub.execute_input":"2025-06-01T04:08:13.266116Z","iopub.status.idle":"2025-06-01T04:08:13.270139Z","shell.execute_reply.started":"2025-06-01T04:08:13.266091Z","shell.execute_reply":"2025-06-01T04:08:13.269397Z"}},"outputs":[{"name":"stdout","text":"Validation Loss: 0.0684\nValidation Accuracy: 0.9835\n","output_type":"stream"}],"execution_count":31}]}