{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport math\nimport time\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:37:09.833807Z","iopub.execute_input":"2025-06-08T13:37:09.834817Z","iopub.status.idle":"2025-06-08T13:37:09.839492Z","shell.execute_reply.started":"2025-06-08T13:37:09.834788Z","shell.execute_reply":"2025-06-08T13:37:09.838531Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"class Config:\n    def __init__(self):\n        \n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        # data Params\n        self.pad_idx = 0\n        self.sos_idx = 1\n        self.eos_idx = 2 \n        self.dummy_vocab = {\n            '<pad>': self.pad_idx,\n            '<sos>': self.sos_idx,\n            '<eos>': self.eos_idx,\n            '0': 3, '1': 4, '2': 5, '3': 6, '4': 7,\n            '5': 8, '6': 9, '7': 10, '8': 11, '9': 12\n        }\n        self.id_to_token = {v: k for k, v in self.dummy_vocab.items()}\n        self.dummy_vocab_size = len(self.dummy_vocab)\n        self.num_samples = 10000 \n        self.max_sequence_length = 20 # including sos/eos/Padding\n\n\n        self.batch_size = 64\n        self.learning_rate = 0.0001\n        self.num_epochs = 10\n        self.clip_grad_norm = 1.0 # gradient clipping value\n\n  \n        self.d_model = 512       \n        self.num_heads = 8       \n        self.num_layers = 3      \n        self.d_ff = 2048         \n        self.dropout_rate = 0.1\n\n        \n        self.model_save_path = 'best_transformer_dummy_model.pt'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:37:09.841182Z","iopub.execute_input":"2025-06-08T13:37:09.841402Z","iopub.status.idle":"2025-06-08T13:37:09.859347Z","shell.execute_reply.started":"2025-06-08T13:37:09.841385Z","shell.execute_reply":"2025-06-08T13:37:09.858637Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:37:09.860943Z","iopub.execute_input":"2025-06-08T13:37:09.861267Z","iopub.status.idle":"2025-06-08T13:37:09.881943Z","shell.execute_reply.started":"2025-06-08T13:37:09.861247Z","shell.execute_reply":"2025-06-08T13:37:09.881063Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        self.d_k = d_model // num_heads\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        self.wq = nn.Linear(d_model, d_model)\n        self.wk = nn.Linear(d_model, d_model)\n        self.wv = nn.Linear(d_model, d_model)\n        self.wo = nn.Linear(d_model, d_model)\n\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        attention_weights = F.softmax(scores, dim=-1)\n        output = torch.matmul(attention_weights, V)\n        return output, attention_weights\n\n    def forward(self, Q, K, V, mask=None):\n        batch_size = Q.size(0)\n        Q = self.wq(Q)\n        K = self.wk(K)\n        V = self.wv(V)\n        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        output = self.wo(output)\n        return output, attention_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:37:09.882919Z","iopub.execute_input":"2025-06-08T13:37:09.883221Z","iopub.status.idle":"2025-06-08T13:37:09.900851Z","shell.execute_reply.started":"2025-06-08T13:37:09.883197Z","shell.execute_reply":"2025-06-08T13:37:09.899866Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"class FeedForwardNetwork(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(FeedForwardNetwork, self).__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.linear2(self.relu(self.linear1(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:37:09.902479Z","iopub.execute_input":"2025-06-08T13:37:09.902792Z","iopub.status.idle":"2025-06-08T13:37:09.923237Z","shell.execute_reply.started":"2025-06-08T13:37:09.902765Z","shell.execute_reply":"2025-06-08T13:37:09.922325Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"class AddNorm(nn.Module):\n    def __init__(self, d_model, dropout_rate=0.1):\n        super(AddNorm, self).__init__()\n        self.norm = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x, sublayer_output):\n        # add residual connection and apply layer norm\n        return self.norm(x + self.dropout(sublayer_output))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:37:09.924310Z","iopub.execute_input":"2025-06-08T13:37:09.924631Z","iopub.status.idle":"2025-06-08T13:37:09.939396Z","shell.execute_reply.started":"2025-06-08T13:37:09.924604Z","shell.execute_reply":"2025-06-08T13:37:09.938753Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1):\n        super(EncoderLayer, self).__init__()\n        self.self_attention = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = FeedForwardNetwork(d_model, d_ff)\n        self.add_norm1 = AddNorm(d_model, dropout_rate)\n        self.add_norm2 = AddNorm(d_model, dropout_rate)\n\n    def forward(self, x, src_mask):\n        attn_output, _ = self.self_attention(x, x, x, mask=src_mask)\n        x = self.add_norm1(x, attn_output)\n        ff_output = self.feed_forward(x)\n        x = self.add_norm2(x, ff_output)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:37:09.940557Z","iopub.execute_input":"2025-06-08T13:37:09.940975Z","iopub.status.idle":"2025-06-08T13:37:09.967159Z","shell.execute_reply.started":"2025-06-08T13:37:09.940947Z","shell.execute_reply":"2025-06-08T13:37:09.966448Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout_rate=0.1):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_len=5000) \n        self.layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ff, dropout_rate)\n            for _ in range(num_layers)\n        ])\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, src, src_mask):\n        x = self.embedding(src)\n        x = self.positional_encoding(x)\n        x = self.dropout(x)\n        for layer in self.layers:\n            x = layer(x, src_mask)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:37:09.984693Z","iopub.execute_input":"2025-06-08T13:37:09.984990Z","iopub.status.idle":"2025-06-08T13:37:09.992410Z","shell.execute_reply.started":"2025-06-08T13:37:09.984968Z","shell.execute_reply":"2025-06-08T13:37:09.991718Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout_rate=0.1):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_len=5000) \n        self.layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ff, dropout_rate)\n            for _ in range(num_layers)\n        ])\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, src, src_mask):\n        x = self.embedding(src)\n        x = self.positional_encoding(x)\n        x = self.dropout(x)\n        for layer in self.layers:\n            x = layer(x, src_mask)\n        return x\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1):\n        super(DecoderLayer, self).__init__()\n        self.masked_self_attention = MultiHeadAttention(d_model, num_heads)\n        self.encoder_attention = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = FeedForwardNetwork(d_model, d_ff)\n        self.add_norm1 = AddNorm(d_model, dropout_rate)\n        self.add_norm2 = AddNorm(d_model, dropout_rate)\n        self.add_norm3 = AddNorm(d_model, dropout_rate)\n\n    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n        attn_output1, _ = self.masked_self_attention(tgt, tgt, tgt, mask=tgt_mask)\n        tgt = self.add_norm1(tgt, attn_output1)\n        attn_output2, _ = self.encoder_attention(tgt, encoder_output, encoder_output, mask=src_mask)\n        tgt = self.add_norm2(tgt, attn_output2)\n        ff_output = self.feed_forward(tgt)\n        tgt = self.add_norm3(tgt, ff_output)\n        return tgt\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout_rate=0.1): \n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_len=5000)\n        self.layers = nn.ModuleList([\n            DecoderLayer(d_model, num_heads, d_ff, dropout_rate)\n            for _ in range(num_layers)\n        ])\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n        x = self.embedding(tgt)\n        x = self.positional_encoding(x)\n        x = self.dropout(x)\n        for layer in self.layers:\n            x = layer(x, encoder_output, tgt_mask, src_mask)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:37:09.993908Z","iopub.execute_input":"2025-06-08T13:37:09.994156Z","iopub.status.idle":"2025-06-08T13:37:10.011655Z","shell.execute_reply.started":"2025-06-08T13:37:09.994137Z","shell.execute_reply":"2025-06-08T13:37:10.010768Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout_rate=0.1, max_len=5000):\n        super(Transformer, self).__init__()\n        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout_rate)\n        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout_rate)\n        self.output_linear = nn.Linear(d_model, tgt_vocab_size)\n        self.pad_idx = 0 \n\n    def generate_src_mask(self, src):\n        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n        return src_mask\n\n    def generate_tgt_mask(self, tgt):\n        seq_len = tgt.size(1)\n        causal_mask = (1 - torch.triu(torch.ones(seq_len, seq_len, device=tgt.device), diagonal=1)).bool()\n        padding_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(2)\n        tgt_mask = causal_mask & padding_mask\n        return tgt_mask\n\n    def forward(self, src, tgt):\n        src_mask = self.generate_src_mask(src)\n        tgt_mask = self.generate_tgt_mask(tgt)\n        encoder_output = self.encoder(src, src_mask)\n        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)\n        output = self.output_linear(decoder_output)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:37:10.012575Z","iopub.execute_input":"2025-06-08T13:37:10.012838Z","iopub.status.idle":"2025-06-08T13:37:10.036050Z","shell.execute_reply.started":"2025-06-08T13:37:10.012818Z","shell.execute_reply":"2025-06-08T13:37:10.035284Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"def tokenize_sequence(sequence_str, vocab, add_sos_eos = False, sos_idx = 1, eos_idx = 2):\n    tokens = []\n    if add_sos_eos:\n        tokens.append(sos_idx)\n    for char in  sequence_str:\n        tokens.append(vocab[char])\n    if add_sos_eos:\n        tokens.append(eos_idx)\n    return tokens\n\ndef detokenize_sequence(sequence_ids, id_to_token, pad_idx = 0,sos_idx = 1, eos_idx =2):\n    return ''.join([id_to_token[idx] for idx in sequence_ids if idx not in [pad_idx, sos_idx, eos_idx]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:37:10.038012Z","iopub.execute_input":"2025-06-08T13:37:10.038258Z","iopub.status.idle":"2025-06-08T13:37:10.055206Z","shell.execute_reply.started":"2025-06-08T13:37:10.038239Z","shell.execute_reply":"2025-06-08T13:37:10.053820Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"class DummyTranslationDataset(Dataset):\n    def __init__(self, num_samples, max_len, vocab, id_to_token, pad_idx, sos_idx, eos_idx):\n        self.num_samples = num_samples\n        self.max_len = max_len\n        self.vocab = vocab\n        self.id_to_token = id_to_token\n        self.pad_idx = pad_idx\n        self.sos_idx = sos_idx\n        self.eos_idx = eos_idx\n        self.data = self._generate_data()\n\n    def _generate_data(self):\n        data = []\n        for _ in range(self.num_samples):\n            src_len = torch.randint(2, self.max_len - 2, (1,)).item()\n            src_sequence_str = ''.join(str(torch.randint(0, 10, (1,)).item()) for _ in range(src_len))\n            tgt_sequence_str = src_sequence_str\n\n            src_tokens = tokenize_sequence(src_sequence_str, self.vocab, add_sos_eos=True,\n                                            sos_idx=self.sos_idx, eos_idx=self.eos_idx)\n            tgt_tokens = tokenize_sequence(tgt_sequence_str, self.vocab, add_sos_eos=True,\n                                            sos_idx=self.sos_idx, eos_idx=self.eos_idx)\n\n            src_padded = src_tokens + [self.pad_idx] * (self.max_len - len(src_tokens))\n            tgt_padded = tgt_tokens + [self.pad_idx] * (self.max_len - len(tgt_tokens))\n\n            src_padded = src_padded[:self.max_len]\n            tgt_padded = tgt_padded[:self.max_len]\n\n            data.append((torch.tensor(src_padded, dtype=torch.long),\n                         torch.tensor(tgt_padded, dtype=torch.long)))\n        return data\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        return self.data[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:37:10.056129Z","iopub.execute_input":"2025-06-08T13:37:10.056378Z","iopub.status.idle":"2025-06-08T13:37:10.084428Z","shell.execute_reply.started":"2025-06-08T13:37:10.056356Z","shell.execute_reply":"2025-06-08T13:37:10.083524Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"def train_epoch(model, dataloader, optimizer, criterion, config):\n    model.train()\n    total_loss = 0\n    for batch_idx, (src, tgt) in enumerate(dataloader):\n        src, tgt = src.to(config.device), tgt.to(config.device)\n\n        tgt_input = tgt[:, :-1]\n        tgt_target = tgt[:, 1:]\n\n        optimizer.zero_grad()\n        output = model(src, tgt_input)\n\n        loss = criterion(output.contiguous().view(-1, output.size(-1)),\n                         tgt_target.contiguous().view(-1))\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), config.clip_grad_norm)\n        optimizer.step()\n        total_loss += loss.item()\n\n        if batch_idx % 100 == 0:\n            print(f\"  Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n\n    return total_loss / len(dataloader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:37:10.085362Z","iopub.execute_input":"2025-06-08T13:37:10.085709Z","iopub.status.idle":"2025-06-08T13:37:10.099986Z","shell.execute_reply.started":"2025-06-08T13:37:10.085677Z","shell.execute_reply":"2025-06-08T13:37:10.099346Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"def evaluate(model, dataloader, criterion, config):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for src, tgt in dataloader:\n            src, tgt = src.to(config.device), tgt.to(config.device)\n\n            tgt_input = tgt[:, :-1]\n            tgt_target = tgt[:, 1:]\n\n            output = model(src, tgt_input)\n            loss = criterion(output.contiguous().view(-1, output.size(-1)),\n                             tgt_target.contiguous().view(-1))\n            total_loss += loss.item()\n    return total_loss / len(dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:37:10.100713Z","iopub.execute_input":"2025-06-08T13:37:10.100928Z","iopub.status.idle":"2025-06-08T13:37:10.122196Z","shell.execute_reply.started":"2025-06-08T13:37:10.100912Z","shell.execute_reply":"2025-06-08T13:37:10.121518Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def generate_sequence(model, src_sequence, config):\n    model.eval()\n    src_tokens = tokenize_sequence(src_sequence, config.dummy_vocab, add_sos_eos=True,\n                                    sos_idx=config.sos_idx, eos_idx=config.eos_idx)\n    src_tensor = torch.tensor(src_tokens, dtype=torch.long, device=config.device).unsqueeze(0)\n\n    tgt_tokens = [config.sos_idx]\n    tgt_tensor = torch.tensor(tgt_tokens, dtype=torch.long, device=config.device).unsqueeze(0)\n\n    for _ in range(config.max_sequence_length):\n        with torch.no_grad():\n            output = model(src_tensor, tgt_tensor)\n            next_token_logits = output[:, -1, :]\n            next_token_id = next_token_logits.argmax(dim=-1).item()\n\n            tgt_tokens.append(next_token_id)\n            tgt_tensor = torch.tensor(tgt_tokens, dtype=torch.long, device=config.device).unsqueeze(0)\n\n            if next_token_id == config.eos_idx:\n                break\n\n    return detokenize_sequence(tgt_tokens, config.id_to_token,\n                               pad_idx=config.pad_idx, sos_idx=config.sos_idx, eos_idx=config.eos_idx)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:37:10.123090Z","iopub.execute_input":"2025-06-08T13:37:10.123351Z","iopub.status.idle":"2025-06-08T13:37:10.143161Z","shell.execute_reply.started":"2025-06-08T13:37:10.123332Z","shell.execute_reply":"2025-06-08T13:37:10.142348Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"class DummyTranslationDataset(Dataset):\n    def __init__(self, num_samples, max_len, vocab, id_to_token, pad_idx, sos_idx, eos_idx):\n        self.num_samples = int(num_samples)\n        self.max_len = max_len\n        self.vocab = vocab\n        self.id_to_token = id_to_token\n        self.pad_idx = pad_idx\n        self.sos_idx = sos_idx\n        self.eos_idx = eos_idx\n        self.data = self._generate_data()\n\n    def _generate_data(self):\n        data = []\n        for _ in range(self.num_samples):\n            src_len = torch.randint(2, self.max_len - 2, (1,)).item()\n            src_sequence_str = ''.join(str(torch.randint(0, 10, (1,)).item()) for _ in range(src_len))\n            tgt_sequence_str = src_sequence_str\n\n            src_tokens = tokenize_sequence(src_sequence_str, self.vocab, add_sos_eos=True,\n                                            sos_idx=self.sos_idx, eos_idx=self.eos_idx)\n            tgt_tokens = tokenize_sequence(tgt_sequence_str, self.vocab, add_sos_eos=True,\n                                            sos_idx=self.sos_idx, eos_idx=self.eos_idx)\n\n            src_padded = src_tokens + [self.pad_idx] * (self.max_len - len(src_tokens))\n            tgt_padded = tgt_tokens + [self.pad_idx] * (self.max_len - len(tgt_tokens))\n\n            src_padded = src_padded[:self.max_len]\n            tgt_padded = tgt_padded[:self.max_len]\n\n            data.append((torch.tensor(src_padded, dtype=torch.long),\n                         torch.tensor(tgt_padded, dtype=torch.long)))\n        return data\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        return self.data[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:37:10.144167Z","iopub.execute_input":"2025-06-08T13:37:10.144504Z","iopub.status.idle":"2025-06-08T13:37:10.165751Z","shell.execute_reply.started":"2025-06-08T13:37:10.144452Z","shell.execute_reply":"2025-06-08T13:37:10.164853Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"def main():\n    config = Config() \n\n    print(f\"Using device: {config.device}\")\n\n    print(\"generating dummy dataset\")\n    train_dataset = DummyTranslationDataset(\n        config.num_samples * 0.8, config.max_sequence_length, \n        config.dummy_vocab, config.id_to_token, config.pad_idx,\n        config.sos_idx, config.eos_idx\n    )\n    val_dataset = DummyTranslationDataset(\n        config.num_samples * 0.2, config.max_sequence_length, \n        config.dummy_vocab, config.id_to_token, config.pad_idx,\n        config.sos_idx, config.eos_idx\n    )\n\n    train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, pin_memory=True)\n    val_dataloader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, pin_memory=True)\n    print(f\"Dataset generated. Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n\n    model = Transformer(\n        src_vocab_size=config.dummy_vocab_size,\n        tgt_vocab_size=config.dummy_vocab_size,\n        d_model=config.d_model,\n        num_layers=config.num_layers,\n        num_heads=config.num_heads,\n        d_ff=config.d_ff,\n        dropout_rate=config.dropout_rate,\n        max_len=config.max_sequence_length\n    ).to(config.device)\n\n    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, betas=(0.9, 0.98), eps=1e-9)\n    criterion = nn.CrossEntropyLoss(ignore_index=config.pad_idx)\n\n    print(\"starting training\")\n    best_val_loss = float('inf')\n\n    for epoch in range(config.num_epochs):\n        start_time = time.time()\n        train_loss = train_epoch(model, train_dataloader, optimizer, criterion, config)\n        val_loss = evaluate(model, val_dataloader, criterion, config)\n        end_time = time.time()\n        epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n\n        print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s\")\n        print(f\"\\tTrain Loss: {train_loss:.3f}\")\n        print(f\"\\tVal Loss: {val_loss:.3f}\")\n\n        # Save best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), config.model_save_path)\n            print(f\"\\t--> Model saved to {config.model_save_path}: New best validation loss {best_val_loss:.3f}\")\n\n    print(\"training complete\")\n\n\n    print(\" testing Inference\")\n    if os.path.exists(config.model_save_path):\n        model.load_state_dict(torch.load(config.model_save_path, map_location=config.device))\n        print(\"Loaded best model for inference.\")\n    else:\n        print(\"No saved model found. Inference might not be optimal.\")\n\n    model.eval()\n\n    test_src_sequences = [\"123\", \"98765\", \"001\", \"5\", \"42\", \"73\"]\n    for src_seq in test_src_sequences:\n        generated_tgt = generate_sequence(model, src_seq, config)\n        print(f\"Source: '{src_seq}' | Generated Target: '{generated_tgt}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:37:10.168384Z","iopub.execute_input":"2025-06-08T13:37:10.168641Z","iopub.status.idle":"2025-06-08T13:37:10.187309Z","shell.execute_reply.started":"2025-06-08T13:37:10.168621Z","shell.execute_reply":"2025-06-08T13:37:10.186231Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:37:10.188335Z","iopub.execute_input":"2025-06-08T13:37:10.188653Z","iopub.status.idle":"2025-06-08T13:38:47.897623Z","shell.execute_reply.started":"2025-06-08T13:37:10.188633Z","shell.execute_reply":"2025-06-08T13:38:47.896821Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\ngenerating dummy dataset\nDataset generated. Train samples: 8000, Val samples: 2000\nstarting training\n  Batch 0/125, Loss: 2.8255\n  Batch 100/125, Loss: 0.3366\nEpoch: 01 | Time: 0m 9s\n\tTrain Loss: 1.039\n\tVal Loss: 0.170\n\t--> Model saved to best_transformer_dummy_model.pt: New best validation loss 0.170\n  Batch 0/125, Loss: 0.2305\n  Batch 100/125, Loss: 0.0881\nEpoch: 02 | Time: 0m 9s\n\tTrain Loss: 0.123\n\tVal Loss: 0.022\n\t--> Model saved to best_transformer_dummy_model.pt: New best validation loss 0.022\n  Batch 0/125, Loss: 0.0858\n  Batch 100/125, Loss: 0.0874\nEpoch: 03 | Time: 0m 10s\n\tTrain Loss: 0.046\n\tVal Loss: 0.004\n\t--> Model saved to best_transformer_dummy_model.pt: New best validation loss 0.004\n  Batch 0/125, Loss: 0.0618\n  Batch 100/125, Loss: 0.0108\nEpoch: 04 | Time: 0m 10s\n\tTrain Loss: 0.031\n\tVal Loss: 0.018\n  Batch 0/125, Loss: 0.0488\n  Batch 100/125, Loss: 0.0154\nEpoch: 05 | Time: 0m 10s\n\tTrain Loss: 0.023\n\tVal Loss: 0.002\n\t--> Model saved to best_transformer_dummy_model.pt: New best validation loss 0.002\n  Batch 0/125, Loss: 0.0216\n  Batch 100/125, Loss: 0.0155\nEpoch: 06 | Time: 0m 10s\n\tTrain Loss: 0.019\n\tVal Loss: 0.006\n  Batch 0/125, Loss: 0.0099\n  Batch 100/125, Loss: 0.0152\nEpoch: 07 | Time: 0m 9s\n\tTrain Loss: 0.017\n\tVal Loss: 0.002\n\t--> Model saved to best_transformer_dummy_model.pt: New best validation loss 0.002\n  Batch 0/125, Loss: 0.0018\n  Batch 100/125, Loss: 0.0045\nEpoch: 08 | Time: 0m 9s\n\tTrain Loss: 0.015\n\tVal Loss: 0.004\n  Batch 0/125, Loss: 0.0222\n  Batch 100/125, Loss: 0.0086\nEpoch: 09 | Time: 0m 9s\n\tTrain Loss: 0.014\n\tVal Loss: 0.008\n  Batch 0/125, Loss: 0.0381\n  Batch 100/125, Loss: 0.0012\nEpoch: 10 | Time: 0m 9s\n\tTrain Loss: 0.011\n\tVal Loss: 0.004\ntraining complete\n testing Inference\nLoaded best model for inference.\nSource: '123' | Generated Target: '123'\nSource: '98765' | Generated Target: '98765'\nSource: '001' | Generated Target: '001'\nSource: '5' | Generated Target: '5'\nSource: '42' | Generated Target: '42'\nSource: '73' | Generated Target: '73'\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}