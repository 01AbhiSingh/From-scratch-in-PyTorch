{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":76002,"sourceType":"datasetVersion","datasetId":42853}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T16:45:43.608907Z","iopub.execute_input":"2025-02-03T16:45:43.609127Z","iopub.status.idle":"2025-02-03T16:45:44.163888Z","shell.execute_reply.started":"2025-02-03T16:45:43.609106Z","shell.execute_reply":"2025-02-03T16:45:44.163081Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/mnist-original/mnist-original.mat\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install scipy -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T16:46:05.781459Z","iopub.execute_input":"2025-02-03T16:46:05.781801Z","iopub.status.idle":"2025-02-03T16:46:10.563004Z","shell.execute_reply.started":"2025-02-03T16:46:05.781773Z","shell.execute_reply":"2025-02-03T16:46:10.561580Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport scipy.io as sc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T16:46:22.438566Z","iopub.execute_input":"2025-02-03T16:46:22.438939Z","iopub.status.idle":"2025-02-03T16:46:28.670840Z","shell.execute_reply.started":"2025-02-03T16:46:22.438910Z","shell.execute_reply":"2025-02-03T16:46:28.669965Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T16:46:28.671745Z","iopub.execute_input":"2025-02-03T16:46:28.672212Z","iopub.status.idle":"2025-02-03T16:46:28.754090Z","shell.execute_reply.started":"2025-02-03T16:46:28.672181Z","shell.execute_reply":"2025-02-03T16:46:28.752877Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"mat = sc.loadmat('/kaggle/input/mnist-original/mnist-original.mat')\nmnist_data = torch.tensor(mat[\"data\"].T,dtype = torch.float32)\nmnist_label = torch.tensor(mat[\"label\"][0],dtype = torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T16:46:29.022264Z","iopub.execute_input":"2025-02-03T16:46:29.022593Z","iopub.status.idle":"2025-02-03T16:46:29.558971Z","shell.execute_reply.started":"2025-02-03T16:46:29.022570Z","shell.execute_reply":"2025-02-03T16:46:29.558169Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class dataset(Dataset):\n    def __init__(self, data,labels):\n        self.data = data\n        self. labels = labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        label = self.labels[idx]\n        return sample, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T16:46:30.065279Z","iopub.execute_input":"2025-02-03T16:46:30.065580Z","iopub.status.idle":"2025-02-03T16:46:30.070413Z","shell.execute_reply.started":"2025-02-03T16:46:30.065557Z","shell.execute_reply":"2025-02-03T16:46:30.069615Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"mnist_dataset = dataset(mnist_data, mnist_label)\n\n# Create DataLoader\ntrain_loader = DataLoader(mnist_dataset, batch_size=128, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T16:46:31.236199Z","iopub.execute_input":"2025-02-03T16:46:31.236528Z","iopub.status.idle":"2025-02-03T16:46:31.242162Z","shell.execute_reply.started":"2025-02-03T16:46:31.236502Z","shell.execute_reply":"2025-02-03T16:46:31.241189Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class VAE(nn.Module):\n    def __init__(self, input_dim=784, hidden_dim=500, latent_dim=30):\n        super(VAE, self).__init__()\n\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, latent_dim * 2)  # Output both μ and log(σ^2)\n        )\n\n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, input_dim),\n            nn.Sigmoid()  # Output values in [0,1]\n        )\n\n    def reparameterize(self, mu, log_var):\n        \"\"\"Reparameterization trick: z = mu + sigma * epsilon\"\"\"\n        log_var = torch.clamp(log_var, min=-10, max=10)  # Avoid extreme values\n        std = torch.exp(0.5 * log_var)\n        epsilon = torch.randn_like(std)\n        return mu + std * epsilon\n\n    def forward(self, x):\n        # Flatten input\n        x = x.view(-1, 784)\n\n        # Encoder: Get μ and log(σ^2)\n        h = self.encoder(x)\n        mu, log_var = torch.chunk(h, 2, dim=1)  # Split into two parts\n\n        # Sample latent vector\n        z = self.reparameterize(mu, log_var)\n\n        # Decoder: Reconstruct x\n        x_recon = self.decoder(z)\n        return x_recon, mu, log_var\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T16:46:34.317257Z","iopub.execute_input":"2025-02-03T16:46:34.317562Z","iopub.status.idle":"2025-02-03T16:46:34.324735Z","shell.execute_reply.started":"2025-02-03T16:46:34.317539Z","shell.execute_reply":"2025-02-03T16:46:34.323891Z"},"_kg_hide-input":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"model = VAE().to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T16:46:37.220957Z","iopub.execute_input":"2025-02-03T16:46:37.221290Z","iopub.status.idle":"2025-02-03T16:46:37.464348Z","shell.execute_reply.started":"2025-02-03T16:46:37.221268Z","shell.execute_reply":"2025-02-03T16:46:37.463575Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def loss_function(x_recon, x, mu, log_var):\n    recon_loss = nn.functional.mse_loss(x_recon, x.view(-1, 784), reduction=\"sum\")\n    kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())  # KL divergence\n    return recon_loss + kl_div","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T16:46:38.371798Z","iopub.execute_input":"2025-02-03T16:46:38.372144Z","iopub.status.idle":"2025-02-03T16:46:38.377069Z","shell.execute_reply.started":"2025-02-03T16:46:38.372121Z","shell.execute_reply":"2025-02-03T16:46:38.376035Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"num_epochs = 20\ncriterion = nn.BCELoss(reduction='sum')  # BCE loss\n\nfor epoch in range(num_epochs):\n    epoch_recon_loss = 0.0\n    epoch_kl_loss = 0.0\n    num_batches = len(train_loader)\n\n    for images, _ in train_loader:\n        # Normalize images to [0, 1] and flatten\n        images = images.view(-1, 784).to(device) / 255.0\n        \n        # Forward pass\n        outputs, mu, log_var = model(images)\n        \n        # Compute losses\n        reconstruction_loss = criterion(outputs, images)\n        kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n\n        # Accumulate losses\n        epoch_recon_loss += reconstruction_loss.item()\n        epoch_kl_loss += kl_divergence.item()\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss = reconstruction_loss + kl_divergence\n        loss.backward()\n        optimizer.step()\n    \n    # Print average losses for the epoch\n    avg_recon_loss = epoch_recon_loss / num_batches\n    avg_kl_loss = epoch_kl_loss / num_batches\n    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n          f\"Recon Loss: {avg_recon_loss:.4f}, \"\n          f\"KL Loss: {avg_kl_loss:.4f}, \"\n          f\"Total Loss: {avg_recon_loss + avg_kl_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T16:49:23.576706Z","iopub.execute_input":"2025-02-03T16:49:23.577126Z","iopub.status.idle":"2025-02-03T16:50:05.901591Z","shell.execute_reply.started":"2025-02-03T16:49:23.577099Z","shell.execute_reply":"2025-02-03T16:50:05.900664Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/20], Recon Loss: 17972.8176, KL Loss: 2195.5059, Total Loss: 20168.3235\nEpoch [2/20], Recon Loss: 12146.8802, KL Loss: 3110.1204, Total Loss: 15257.0006\nEpoch [3/20], Recon Loss: 11137.1394, KL Loss: 3286.0195, Total Loss: 14423.1589\nEpoch [4/20], Recon Loss: 10715.4163, KL Loss: 3343.7575, Total Loss: 14059.1738\nEpoch [5/20], Recon Loss: 10482.8356, KL Loss: 3371.8985, Total Loss: 13854.7341\nEpoch [6/20], Recon Loss: 10334.1455, KL Loss: 3387.4702, Total Loss: 13721.6156\nEpoch [7/20], Recon Loss: 10232.6645, KL Loss: 3394.9929, Total Loss: 13627.6574\nEpoch [8/20], Recon Loss: 10149.4929, KL Loss: 3400.6267, Total Loss: 13550.1196\nEpoch [9/20], Recon Loss: 10089.4172, KL Loss: 3406.0163, Total Loss: 13495.4336\nEpoch [10/20], Recon Loss: 10037.3096, KL Loss: 3408.1628, Total Loss: 13445.4724\nEpoch [11/20], Recon Loss: 10001.7252, KL Loss: 3413.3609, Total Loss: 13415.0861\nEpoch [12/20], Recon Loss: 9961.1294, KL Loss: 3412.1334, Total Loss: 13373.2628\nEpoch [13/20], Recon Loss: 9930.8396, KL Loss: 3415.5208, Total Loss: 13346.3605\nEpoch [14/20], Recon Loss: 9906.4343, KL Loss: 3412.4018, Total Loss: 13318.8361\nEpoch [15/20], Recon Loss: 9882.4366, KL Loss: 3412.8258, Total Loss: 13295.2624\nEpoch [16/20], Recon Loss: 9859.2068, KL Loss: 3409.5389, Total Loss: 13268.7457\nEpoch [17/20], Recon Loss: 9839.7985, KL Loss: 3408.5126, Total Loss: 13248.3111\nEpoch [18/20], Recon Loss: 9821.3170, KL Loss: 3411.6979, Total Loss: 13233.0149\nEpoch [19/20], Recon Loss: 9804.6478, KL Loss: 3406.0352, Total Loss: 13210.6830\nEpoch [20/20], Recon Loss: 9790.1371, KL Loss: 3407.6276, Total Loss: 13197.7647\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}