{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":268736,"sourceType":"datasetVersion","datasetId":112480}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:24:15.761323Z","iopub.execute_input":"2025-06-21T06:24:15.761579Z","iopub.status.idle":"2025-06-21T06:24:15.765677Z","shell.execute_reply.started":"2025-06-21T06:24:15.761561Z","shell.execute_reply":"2025-06-21T06:24:15.765116Z"}},"outputs":[],"execution_count":118},{"cell_type":"code","source":"class PatchEmbed(nn.Module):\n    def __init__(self,img_size = 224, patch_size = 16, in_channels = 3, embed_dims = 768):\n        super().__init__()\n        self.img_size= img_size\n        self.patch_size = patch_size\n        self.in_channels = in_channels\n        self.embed_dims = 768\n        self.num_patches = (img_size//patch_size)**2\n\n        self.proj = nn.Conv2d(in_channels =  in_channels,\n                          out_channels = embed_dims,\n                          kernel_size = patch_size, \n                          stride = patch_size)\n\n    def forward(self, x):\n        x = self.proj(x)\n        x = x.flatten(2)\n        x = x.transpose(1,2)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:24:15.767874Z","iopub.execute_input":"2025-06-21T06:24:15.768047Z","iopub.status.idle":"2025-06-21T06:24:15.792412Z","shell.execute_reply.started":"2025-06-21T06:24:15.768034Z","shell.execute_reply":"2025-06-21T06:24:15.791675Z"}},"outputs":[],"execution_count":119},{"cell_type":"code","source":"class clsToken(nn.Module):\n    def __init__(self, embed_dims = 768, cls = 1):\n        super().__init__()\n        self.embed_dims = embed_dims\n        self.cls_token = nn.Parameter(torch.zeros(1,1, embed_dims))\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        cls_token = self.cls_token.expand(batch_size, -1, -1)\n        x = torch.cat((cls_token,x), dim = 1)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:24:15.793627Z","iopub.execute_input":"2025-06-21T06:24:15.794096Z","iopub.status.idle":"2025-06-21T06:24:15.807998Z","shell.execute_reply.started":"2025-06-21T06:24:15.794055Z","shell.execute_reply":"2025-06-21T06:24:15.807369Z"}},"outputs":[],"execution_count":120},{"cell_type":"code","source":"class PositionalEmbedding(nn.Module):\n    def __init__(self, num_patches, embed_dims):\n        super().__init__()\n        self.num_patches = num_patches +1 \n        self.embed_dims = embed_dims\n\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, self.embed_dims)) \n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n\n    def forward (self, x):\n        x = x+self.pos_embed\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:24:15.808682Z","iopub.execute_input":"2025-06-21T06:24:15.808920Z","iopub.status.idle":"2025-06-21T06:24:15.824683Z","shell.execute_reply.started":"2025-06-21T06:24:15.808900Z","shell.execute_reply":"2025-06-21T06:24:15.824169Z"}},"outputs":[],"execution_count":121},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dims, num_heads):\n        super().__init__()\n        self.embed_dims = embed_dims\n        self.num_heads = num_heads\n\n        assert embed_dims % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n\n        self.head_dim = embed_dims // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.qkv = nn.Linear(embed_dims, embed_dims * 3, bias = False)\n        self.proj = nn.Linear(embed_dims, embed_dims)\n        self.attn_dropout = nn.Dropout(0.0)\n        self.proj_dropout = nn.Dropout(0.0)\n\n    def forward(self, x):\n        batch_size, num_tokens, embed_dims = x.shape\n        qkv= self.qkv(x).reshape(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n        q, k ,v = qkv.permute(2, 0, 3, 1, 4) \n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n\n        #softmax to get attn prob\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_dropout(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(batch_size, num_tokens, embed_dims)\n\n        x = self.proj(x)\n        x = self.proj_dropout(x)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:24:15.826282Z","iopub.execute_input":"2025-06-21T06:24:15.826518Z","iopub.status.idle":"2025-06-21T06:24:15.839504Z","shell.execute_reply.started":"2025-06-21T06:24:15.826505Z","shell.execute_reply":"2025-06-21T06:24:15.838938Z"}},"outputs":[],"execution_count":122},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, embed_dims, mlp_ratio=4.0, dropout_rate=0.0):\n        super().__init__()\n        self.embed_dims = embed_dims\n        self.hidden_dim = int(embed_dims * mlp_ratio)\n        self.fc1 = nn.Linear(embed_dims, self.hidden_dim)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(self.hidden_dim, embed_dims)\n        self.dropout = nn.Dropout(dropout_rate)\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:24:15.840217Z","iopub.execute_input":"2025-06-21T06:24:15.840458Z","iopub.status.idle":"2025-06-21T06:24:15.858423Z","shell.execute_reply.started":"2025-06-21T06:24:15.840439Z","shell.execute_reply":"2025-06-21T06:24:15.857857Z"}},"outputs":[],"execution_count":123},{"cell_type":"code","source":"class transformerEncoder(nn.Module):\n    def __init__(self, embed_dims,num_heads,  mlp_ratio = 4.0, dropout_rate = 0.0):\n        super().__init__()\n        self.embed_dims = embed_dims\n        self.num_heads = num_heads\n        self.mlp_ratio = mlp_ratio\n        self.dropout_rate = dropout_rate\n\n        self.norm1 = nn.LayerNorm(embed_dims)\n        self.attn = MultiHeadAttention(\n            embed_dims=embed_dims, \n            num_heads=num_heads\n        ) \n\n        self.norm2 = nn.LayerNorm(embed_dims)\n        self.mlp = MLP(\n            embed_dims=embed_dims, \n            mlp_ratio=mlp_ratio, \n            dropout_rate=dropout_rate\n        )\n\n    def forward(self, x):\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:24:15.859010Z","iopub.execute_input":"2025-06-21T06:24:15.859258Z","iopub.status.idle":"2025-06-21T06:24:15.876937Z","shell.execute_reply.started":"2025-06-21T06:24:15.859244Z","shell.execute_reply":"2025-06-21T06:24:15.876445Z"}},"outputs":[],"execution_count":124},{"cell_type":"code","source":"class VisionTransformer(nn.Module):\n    def __init__(self, \n                 img_size=224, \n                 patch_size=16, \n                 in_channels=3, \n                 num_classes=1000, \n                 embed_dims=768, \n                 num_layers=12,\n                 num_heads=12,   \n                 mlp_ratio=4.0, \n                 dropout_rate=0.1): \n        super().__init__()\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        self.embed_dims = embed_dims\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.mlp_ratio = mlp_ratio\n        self.dropout_rate = dropout_rate\n\n        self.patch_embed = PatchEmbed(img_size, patch_size, in_channels, embed_dims)\n        self.cls_token_module = clsToken(embed_dims)\n        num_patches = self.patch_embed.num_patches\n        self.pos_embed_module = PositionalEmbedding(num_patches, embed_dims)\n        self.pos_dropout = nn.Dropout(dropout_rate)\n        self.transformer_blocks = nn.ModuleList([\n            transformerEncoder(\n                embed_dims=embed_dims,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                dropout_rate=dropout_rate\n            )\n            for _ in range(num_layers)\n        ])\n        self.norm = nn.LayerNorm(embed_dims)\n        self.head = nn.Linear(embed_dims, num_classes)\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.trunc_normal_(m.weight, std=0.02)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.LayerNorm):\n                nn.init.zeros_(m.bias)\n                nn.init.ones_(m.weight)\n\n    def forward(self, x):\n\n        x = self.patch_embed(x) \n        x = self.cls_token_module(x)\n        x = self.pos_embed_module(x)\n        x = self.pos_dropout(x)\n        for block in self.transformer_blocks:\n            x = block(x)\n        x = self.norm(x)\n        cls_token_output = x[:, 0] \n        logits = self.head(cls_token_output)\n\n        return logits\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:24:15.924200Z","iopub.execute_input":"2025-06-21T06:24:15.924391Z","iopub.status.idle":"2025-06-21T06:24:15.932329Z","shell.execute_reply.started":"2025-06-21T06:24:15.924376Z","shell.execute_reply":"2025-06-21T06:24:15.931798Z"}},"outputs":[],"execution_count":125},{"cell_type":"code","source":"IMG_SIZE = 224\nPATCH_SIZE = 16\nEMBED_DIMS= 768\nNUM_HEADS = 12\nNUM_LAYERS = 12 \nNUM_CLASSES = 37 \nBATCH_SIZE = 4\n\ndummy_images = torch.randn(BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE) \n\n\nvit_model = VisionTransformer(\n    img_size=IMG_SIZE,\n    patch_size=PATCH_SIZE,\n    in_channels=3,\n    num_classes=NUM_CLASSES,\n    embed_dims=EMBED_DIMS,\n    num_layers=NUM_LAYERS,\n    num_heads=NUM_HEADS,\n    mlp_ratio=4.0,\n    dropout_rate=0.1\n)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvit_model.to(device)\ndummy_images = dummy_images.to(device)\noutput_logits = vit_model(dummy_images)\n\nprint(f\"Input image shape: {dummy_images.shape}\")\nprint(f\"Final output logits shape: {output_logits.shape}\")\ntotal_params = sum(p.numel() for p in vit_model.parameters() if p.requires_grad)\nprint(f\"Total learnable parameters: {total_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:24:15.933465Z","iopub.execute_input":"2025-06-21T06:24:15.933722Z","iopub.status.idle":"2025-06-21T06:24:17.398893Z","shell.execute_reply.started":"2025-06-21T06:24:15.933708Z","shell.execute_reply":"2025-06-21T06:24:17.398324Z"}},"outputs":[{"name":"stdout","text":"Input image shape: torch.Size([4, 3, 224, 224])\nFinal output logits shape: torch.Size([4, 37])\nTotal learnable parameters: 85,799,461\n","output_type":"stream"}],"execution_count":126},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}