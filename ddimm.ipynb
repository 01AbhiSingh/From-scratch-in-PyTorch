{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nfrom tqdm import tqdm\nimport math\nimport numpy as np\nimport os\nfrom PIL import Image\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:33:29.002823Z","iopub.execute_input":"2025-06-10T17:33:29.003329Z","iopub.status.idle":"2025-06-10T17:33:29.007929Z","shell.execute_reply.started":"2025-06-10T17:33:29.003302Z","shell.execute_reply":"2025-06-10T17:33:29.007024Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:33:29.009385Z","iopub.execute_input":"2025-06-10T17:33:29.009763Z","iopub.status.idle":"2025-06-10T17:33:29.024874Z","shell.execute_reply.started":"2025-06-10T17:33:29.009747Z","shell.execute_reply":"2025-06-10T17:33:29.024220Z"}},"outputs":[],"execution_count":88},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, dim: int):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, t_tensor: torch.Tensor) -> torch.Tensor: # Input parameter is t_tensor\n        if t_tensor.dim() == 0:\n            t_tensor = t_tensor.unsqueeze(0)\n        \n        half_dim = self.dim // 2\n        embeddings = math.log(10000) / (half_dim - 1)\n        # Ensure 'embeddings' are created on the correct device\n        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n        embeddings = t_tensor.unsqueeze(1) * embeddings.unsqueeze(0) # Use t_tensor here\n        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n        return embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:33:29.025534Z","iopub.execute_input":"2025-06-10T17:33:29.025779Z","iopub.status.idle":"2025-06-10T17:33:29.039537Z","shell.execute_reply.started":"2025-06-10T17:33:29.025758Z","shell.execute_reply":"2025-06-10T17:33:29.038878Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, time_emb_dim: int, downsample: bool):\n        super().__init__()\n        self.time_mlp = nn.Linear(time_emb_dim, out_channels)\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.norm1 = nn.GroupNorm(8, out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.norm2 = nn.GroupNorm(8, out_channels)\n        self.activation = nn.SiLU()\n\n        if downsample:\n            self.pool = nn.MaxPool2d(2)\n        else:\n            self.pool = nn.Identity()\n\n    def forward(self, x: torch.Tensor, t_emb: torch.Tensor) -> torch.Tensor:\n        h = self.activation(self.norm1(self.conv1(x)))\n        time_embedding = self.activation(self.time_mlp(t_emb))\n        h = h + time_embedding.unsqueeze(-1).unsqueeze(-1)\n        h = self.activation(self.norm2(self.conv2(h)))\n        return self.pool(h)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:33:29.040643Z","iopub.execute_input":"2025-06-10T17:33:29.040881Z","iopub.status.idle":"2025-06-10T17:33:29.057105Z","shell.execute_reply.started":"2025-06-10T17:33:29.040867Z","shell.execute_reply":"2025-06-10T17:33:29.056436Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"class UpBlock(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, time_emb_dim: int):\n        super().__init__()\n        self.time_mlp = nn.Linear(time_emb_dim, out_channels)\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.norm1 = nn.GroupNorm(8, out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.norm2 = nn.GroupNorm(8, out_channels)\n        self.activation = nn.SiLU()\n        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n\n    def forward(self, x: torch.Tensor, skip_x: torch.Tensor, t_emb: torch.Tensor) -> torch.Tensor:\n        x = self.upsample(x)\n        x = torch.cat([x, skip_x], dim=1) \n        h = self.activation(self.norm1(self.conv1(x)))\n        time_embedding = self.activation(self.time_mlp(t_emb))\n        h = h + time_embedding.unsqueeze(-1).unsqueeze(-1)\n        h = self.activation(self.norm2(self.conv2(h)))\n        return h\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels: int = 1, out_channels: int = 1, time_emb_dim: int = 256):\n        super().__init__()\n        self.time_mlp = PositionalEncoding(time_emb_dim)\n\n        self.inc = nn.Conv2d(in_channels, 64, 3, padding=1)\n        self.down1 = ConvBlock(64, 128, time_emb_dim, downsample=True)\n        self.down2 = ConvBlock(128, 256, time_emb_dim, downsample=True)\n\n        self.bot1 = nn.Conv2d(256, 256, 3, padding=1)\n        self.bot2 = nn.Conv2d(256, 256, 3, padding=1)\n\n        self.up1 = UpBlock(256 + 128, 128, time_emb_dim) \n        self.up2 = UpBlock(128 + 64, 64, time_emb_dim)\n        self.outc = nn.Conv2d(64, out_channels, 1)\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n        t_emb = self.time_mlp(t)\n\n        x1 = F.relu(self.inc(x))\n        x2 = self.down1(x1, t_emb)\n        x3 = self.down2(x2, t_emb)\n\n        x3 = F.relu(self.bot1(x3))\n        x3 = F.relu(self.bot2(x3))\n\n        x = self.up1(x3, x2, t_emb)\n        x = self.up2(x, x1, t_emb)\n\n        output = self.outc(x)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:33:29.057876Z","iopub.execute_input":"2025-06-10T17:33:29.058081Z","iopub.status.idle":"2025-06-10T17:33:29.075132Z","shell.execute_reply.started":"2025-06-10T17:33:29.058059Z","shell.execute_reply":"2025-06-10T17:33:29.074523Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"class DDIMScheduler:\n    def __init__(self, timesteps: int = 1000, beta_start: float = 0.0001, beta_end: float = 0.02):\n        self.timesteps = timesteps\n        # Ensure all pre-computed tensors are on the correct device\n        self.betas = torch.linspace(beta_start, beta_end, timesteps, dtype=torch.float32).to(device)\n        self.alphas = 1.0 - self.betas\n        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0).to(device) # Move to device\n        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0).to(device) # Move to device\n        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod).to(device) # Move to device\n        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod).to(device) # Move to device\n\n    def add_noise(self, original_samples: torch.Tensor, noise: torch.Tensor, timesteps: torch.Tensor) -> torch.Tensor:\n        # timesteps_tensor is already on device from the training loop\n        sqrt_alpha_prod_t = self.sqrt_alphas_cumprod[timesteps].view(-1, 1, 1, 1) # timesteps already on device\n        sqrt_one_minus_alpha_prod_t = self.sqrt_one_minus_alphas_cumprod[timesteps].view(-1, 1, 1, 1) # timesteps already on device\n\n        noisy_samples = sqrt_alpha_prod_t * original_samples + sqrt_one_minus_alpha_prod_t * noise\n        return noisy_samples\n\n    def get_ddim_sampling_timesteps(self, num_inference_steps: int) -> torch.Tensor:\n        assert num_inference_steps <= self.timesteps, \\\n            f\"num_inference_steps ({num_inference_steps}) cannot be greater than total timesteps ({self.timesteps})\"\n        \n        skip = self.timesteps // num_inference_steps\n        timesteps = list(range(0, self.timesteps, skip))\n        if timesteps[-1] != self.timesteps - 1:\n            timesteps.append(self.timesteps - 1)\n        \n        return torch.tensor(timesteps[::-1], dtype=torch.long).to(device) # Ensure timesteps are on device\n\n    def ddim_step(self, model_output: torch.Tensor, timestep: torch.Tensor, sample: torch.Tensor, eta: float = 0.0) -> tuple[torch.Tensor, torch.Tensor]:\n        # timestep is already on device from the sampling loop\n        alpha_prod_t = self.alphas_cumprod[timestep]\n        alpha_prod_t_prev = self.alphas_cumprod_prev[timestep]\n\n        sigma_t = eta * torch.sqrt((1 - alpha_prod_t_prev) / (1 - alpha_prod_t)) * \\\n                    torch.sqrt(1 - alpha_prod_t / alpha_prod_t_prev)\n\n        pred_x0 = (sample - torch.sqrt(1 - alpha_prod_t) * model_output) / torch.sqrt(alpha_prod_t)\n        dir_xt = torch.sqrt(1 - alpha_prod_t_prev - sigma_t**2) * model_output\n        \n        noise_term = torch.randn_like(sample) if eta > 0 else torch.zeros_like(sample)\n\n        prev_sample = torch.sqrt(alpha_prod_t_prev) * pred_x0 + dir_xt + sigma_t * noise_term\n        \n        return prev_sample, pred_x0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:33:29.120975Z","iopub.execute_input":"2025-06-10T17:33:29.121179Z","iopub.status.idle":"2025-06-10T17:33:29.130135Z","shell.execute_reply.started":"2025-06-10T17:33:29.121156Z","shell.execute_reply":"2025-06-10T17:33:29.129451Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"class DiffusionPipeline:\n    def __init__(self, model: nn.Module, scheduler: DDIMScheduler, image_size: int, \n                 dataset: Dataset, batch_size: int, lr: float, epochs: int, \n                 save_dir: str = \"ddim_pipeline_results\"):\n        self.model = model.to(device)\n        self.scheduler = scheduler\n        self.image_size = image_size\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.lr = lr\n        self.epochs = epochs\n        self.save_dir = save_dir\n        \n        os.makedirs(save_dir, exist_ok=True)\n\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr)\n        self.criterion = nn.MSELoss()\n\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5,), (0.5,))\n        ])\n        \n        class TransformedDataset(Dataset):\n            def __init__(self, base_dataset, transform):\n                self.base_dataset = base_dataset\n                self.transform = transform\n            \n            def __len__(self):\n                return len(self.base_dataset)\n            \n            def __getitem__(self, idx):\n                img, _ = self.base_dataset[idx]\n                return self.transform(img)\n\n        self.train_dataset = TransformedDataset(dataset, self.transform)\n        self.dataloader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count()//2)\n    \n    def train(self):\n        self.model.train()\n        for epoch in range(self.epochs):\n            total_loss = 0\n            for batch_idx, images in enumerate(tqdm(self.dataloader, desc=f\"Epoch {epoch+1}/{self.epochs}\")):\n                images = images.to(device)\n                \n                timesteps = torch.randint(0, self.scheduler.timesteps, (images.shape[0],), device=device)\n                noise = torch.randn_like(images).to(device)\n                noisy_images = self.scheduler.add_noise(images, noise, timesteps)\n                \n                self.optimizer.zero_grad()\n                predicted_noise = self.model(noisy_images, timesteps)\n                \n                loss = self.criterion(predicted_noise, noise)\n                loss.backward()\n                self.optimizer.step()\n                \n                total_loss += loss.item()\n            \n            avg_loss = total_loss / len(self.dataloader)\n            print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n\n            if (epoch + 1) % 5 == 0 or epoch == self.epochs - 1:\n                torch.save(self.model.state_dict(), os.path.join(self.save_dir, f\"unet_epoch_{epoch+1}.pt\"))\n                self.generate_samples(num_samples=4, num_inference_steps=50, epoch=epoch+1)\n\n    @torch.no_grad()\n    def generate_samples(self, num_samples: int, num_inference_steps: int, eta: float = 0.0, epoch: int = None):\n        self.model.eval()\n        \n        sample = torch.randn(num_samples, self.model.inc.in_channels, self.image_size, self.image_size).to(device)\n        ddim_timesteps = self.scheduler.get_ddim_sampling_timesteps(num_inference_steps)\n        \n        for i, t in enumerate(tqdm(ddim_timesteps, desc=\"DDIM Sampling\")):\n            model_output = self.model(sample, t.unsqueeze(0).repeat(num_samples))\n            sample, _ = self.scheduler.ddim_step(model_output, t, sample, eta)\n\n        sample = (sample.clamp(-1, 1) + 1) / 2\n        sample = sample.cpu().permute(0, 2, 3, 1).numpy()\n        \n        for i in range(num_samples):\n            img = Image.fromarray((sample[i] * 255).astype(np.uint8).squeeze())\n            filename = f\"generated_sample_{'epoch_' + str(epoch) + '_' if epoch is not None else ''}{i}.png\"\n            img.save(os.path.join(self.save_dir, filename))\n        \n        self.model.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:33:29.131250Z","iopub.execute_input":"2025-06-10T17:33:29.131878Z","iopub.status.idle":"2025-06-10T17:33:29.150805Z","shell.execute_reply.started":"2025-06-10T17:33:29.131862Z","shell.execute_reply":"2025-06-10T17:33:29.150171Z"}},"outputs":[],"execution_count":93},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    image_size = 28\n    in_channels = 1\n    epochs = 20\n    batch_size = 128\n    learning_rate = 1e-4\n    timesteps = 1000\n    num_inference_steps = 50\n    eta = 0.0\n\n    mnist_dataset = MNIST(root=\"./data\", train=True, download=True)\n\n    model = UNet(in_channels=in_channels, out_channels=in_channels, time_emb_dim=256)\n    scheduler = DDIMScheduler(timesteps=timesteps)\n\n    pipeline = DiffusionPipeline(\n        model=model,\n        scheduler=scheduler,\n        image_size=image_size,\n        dataset=mnist_dataset,\n        batch_size=batch_size,\n        lr=learning_rate,\n        epochs=epochs\n    )\n    \n    print(\"Starting DDIM training pipeline...\")\n    pipeline.train()\n    print(\"DDIM training complete.\")\n\n    print(\"Generating final samples with trained model...\")\n    pipeline.generate_samples(num_samples=16, num_inference_steps=num_inference_steps, eta=eta, epoch=\"final\")\n    print(\"Final samples generated and saved.\")\n\n\n    loaded_model = UNet(in_channels=in_channels, out_channels=in_channels, time_emb_dim=256).to(device)\n    loaded_model.load_state_dict(torch.load(\"ddim_pipeline_results/unet_epoch_50.pt\"))\n    print(\"Loaded trained model for further sampling.\")\n    loaded_pipeline = DiffusionPipeline(loaded_model, scheduler, image_size, mnist_dataset, batch_size, learning_rate, epochs)\n    loaded_pipeline.generate_samples(num_samples=16, num_inference_steps=num_inference_steps, eta=eta, epoch=\"loaded_model_test\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:33:29.151620Z","iopub.execute_input":"2025-06-10T17:33:29.151903Z","iopub.status.idle":"2025-06-10T17:50:30.052268Z","shell.execute_reply.started":"2025-06-10T17:33:29.151879Z","shell.execute_reply":"2025-06-10T17:50:30.051231Z"}},"outputs":[{"name":"stdout","text":"Starting DDIM training pipeline...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/20: 100%|██████████| 469/469 [00:49<00:00,  9.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 0.0976\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 100%|██████████| 469/469 [00:51<00:00,  9.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 0.0403\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20: 100%|██████████| 469/469 [00:51<00:00,  9.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 0.0349\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20: 100%|██████████| 469/469 [00:51<00:00,  9.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Loss: 0.0332\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/20: 100%|██████████| 469/469 [00:50<00:00,  9.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Loss: 0.0309\n","output_type":"stream"},{"name":"stderr","text":"DDIM Sampling: 100%|██████████| 51/51 [00:00<00:00, 231.75it/s]\nEpoch 6/20: 100%|██████████| 469/469 [00:50<00:00,  9.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Loss: 0.0298\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/20: 100%|██████████| 469/469 [00:50<00:00,  9.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Loss: 0.0288\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/20: 100%|██████████| 469/469 [00:50<00:00,  9.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Loss: 0.0281\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/20: 100%|██████████| 469/469 [00:50<00:00,  9.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Loss: 0.0278\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/20: 100%|██████████| 469/469 [00:50<00:00,  9.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Loss: 0.0272\n","output_type":"stream"},{"name":"stderr","text":"DDIM Sampling: 100%|██████████| 51/51 [00:00<00:00, 379.75it/s]\nEpoch 11/20: 100%|██████████| 469/469 [00:50<00:00,  9.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11, Loss: 0.0267\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/20: 100%|██████████| 469/469 [00:50<00:00,  9.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12, Loss: 0.0268\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/20: 100%|██████████| 469/469 [00:50<00:00,  9.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13, Loss: 0.0263\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/20: 100%|██████████| 469/469 [00:50<00:00,  9.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14, Loss: 0.0258\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/20: 100%|██████████| 469/469 [00:51<00:00,  9.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15, Loss: 0.0255\n","output_type":"stream"},{"name":"stderr","text":"DDIM Sampling: 100%|██████████| 51/51 [00:00<00:00, 344.01it/s]\nEpoch 16/20: 100%|██████████| 469/469 [00:51<00:00,  9.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16, Loss: 0.0253\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/20: 100%|██████████| 469/469 [00:50<00:00,  9.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17, Loss: 0.0252\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/20: 100%|██████████| 469/469 [00:51<00:00,  9.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18, Loss: 0.0252\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/20: 100%|██████████| 469/469 [00:51<00:00,  9.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19, Loss: 0.0250\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/20: 100%|██████████| 469/469 [00:51<00:00,  9.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20, Loss: 0.0249\n","output_type":"stream"},{"name":"stderr","text":"DDIM Sampling: 100%|██████████| 51/51 [00:00<00:00, 367.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"DDIM training complete.\nGenerating final samples with trained model...\n","output_type":"stream"},{"name":"stderr","text":"DDIM Sampling: 100%|██████████| 51/51 [00:00<00:00, 185.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Final samples generated and saved.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1492404722.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_emb_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ddim_pipeline_results/unet_epoch_50.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded trained model for further sampling.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mloaded_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiffusionPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ddim_pipeline_results/unet_epoch_50.pt'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'ddim_pipeline_results/unet_epoch_50.pt'","output_type":"error"}],"execution_count":94}]}